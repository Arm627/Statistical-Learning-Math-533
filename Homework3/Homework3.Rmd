---
title: "Homework 3"
author: "Brandon Amaral, Monte Davityan, Nicholas Lombardo, Hongkai Lu"
date: '2022-09-16'
output: pdf_document
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```


```{r}
library(tidyverse)
library(ggplot2)
library(knitr)
fire <- read.csv("data/forestfires.csv")
```


```{r}
head(fire)
```


```{r}
kable(summary(fire %>% select(X, Y, month, day, FFMC, DC, ISI)))

kable(summary(fire %>% select(temp, RH, wind, rain, area, DMC)))
```


## 2a
```{r}
fire <- fire %>% mutate(month = as.factor(month),
                day = as.factor(day))
```
```{r}
# Response area (multiple regression)
lm.fit <- lm(area ~ ., data = fire )
#summary(lm.fit)

#par(mfrow=c(2,2))
#plot(lm.fit)

# Response area (multiple regression) (Transformed Model)
lm.fit.transformed <- lm(log(area + 1) ~ ., data = fire )
#summary(lm.fit.transformed)

kable(lm.fit.transformed$coefficients)

par(mfrow=c(2,2))
plot(lm.fit.transformed)
```



Based on the residual plot, this model seems to be a poor model. The residual vs fit plot shows a trend in predictions, normality doesn't seem to met especially at the tails, the scale location plot provides evidence for non- constant variance and some points are higher in leverage.



$$
log(y + 1) = \beta_0 + \beta_1x_1 + \cdots + \beta_nx_n\\
y = e^{\beta_0}e^{\beta_1x_1}\cdots e^{\beta_nx_n} - 1
$$

In general, the individual predictor variable effects the response (area) by leaving all else constant, a one unit increase in that predictor, effects the response (area) multiplicativly by $e^{\beta_i}$. For example: monthaug, leaving all else constant, if the month is august, will multiplicativly increase the area by $e^{0.3274391}$. Another example: ISI, leaving all else constant, a one unit increase in ISI will decrease area by $e^{0.0147970}$.


## 2b
```{r}
# Binary area
fire <- fire %>% 
  mutate(binary_area = if_else(area != 0, "Not zero", "Zero")) %>% 
  mutate(binary_area = as.factor(binary_area))

glm.fit <- glm(binary_area ~ ., family = "binomial", data = fire %>% select(-area))

kable(glm.fit$coefficients)
```


**In general, the individual predictor variable effects the response (area) by leaving all else constant, a one unit increase in that predictor, effects the response (area) by its coefficient estimate. For example: wind, leaving all else constant, with a one unit increase in wind, will increase the log odds of area being non- zero by -0.08036**

## 2c
```{r}
# Predict using LM for rows 15-25 and report MSE:

pred <- predict(lm.fit, newdata = fire[15:25,])
pred.transformed <- predict(lm.fit.transformed, newdata = fire[15:25,])

actual <- fire[15:25,] %>% select(area)

MSE <- mean(unlist((pred - actual)^2))
MSE
MSE.transformed <- mean(unlist((pred.transformed - actual)^2))
MSE.transformed
```


## 2d

```{r}
fire2 <- fire %>% mutate(area = log1p(area))
```

```{r}
#set.seed(123)

training_ind <- sample(1:517, floor(.8*517), replace = FALSE)

train <- fire2[training_ind, c("temp","RH","wind","rain", "area")]
val <- fire2[-training_ind, c("temp","RH","wind","rain", "area")]

model.combos <- function(model, 
                         response, 
                         predictors, 
                         training, 
                         validation, 
                         null.model = TRUE,
                         debug = FALSE) {
  "This function iterates through all possible combos (2^n)-1 of 
    predictor variables and outputs the model with the best validation 
    MSE
  
    Inputs:
      - model: A supervised regression learning model object (ie. lm)
          Expecting the function to have formula and data parameters
      - response: A string. The name of the response variable
      - predictors: A vector of strings. The list of names of the     
          predictor variables
      - training: A data frame. The training set
      - validation: A data frame. The validation set
      - null.model: A boolean. (Optional). If TRUE, will compare the initial best
          model as the null model (which is just the average). If set to FALSE,
          the intial best model will be null and only the (2^n)-1 combos will be           considered
      - debug: A boolean. (Optional). If TRUE print out the best.MSE and 
          best.predictions as they are being calculated
      
    Output:
      - best.predictors: A list. Containing the names of the best 
        predictors (as measured by lowest validation MSE)
      - best.mse: A number. The best MSE among the models
  "
  # Combinations is made from this code: 
  # https://stackoverflow.com/questions/40049313/generate-all-combinations-of-all-lengths-in-r-from-a-vector
  # It generates the combinations of predictors as a list of strings
  combinations <- do.call("c", lapply(seq_along(predictors), function(i) combn(predictors, i, FUN = list)))
  
  if (debug) {
    print(combinations)
  }
  
  # The validation response
  actual <- validation[[response]]
  
  # If null.model is TRUE, compare to the null model (the average)
  if (null.model) {
    average.model <- mean(training[[response]])
    best.MSE <- mean((average.model - actual)^2)
    best.predictions <- c("null model")
    
    if (debug) {
      print(best.MSE)
      print("null")
    }
  }
  # If null.model is FALSE, compare only the (2^n)-1 models
  else {
    best.MSE <- NA
    best.predictions <- NA
  }
  
  
  # Loop through the combinations
  for (combo in combinations) {
    # Make the formula from the combos and the response
    formula <- reformulate(combo, response = response)
    # Fit the model using the given model and training data
    model.fit <- model(formula = formula, data = training)
    # Make predictions on the validation set
    preds <- predict(model.fit, newdata = validation)
    # Calculate validation MSE using the predictions
    MSE.validation <- mean(unlist((preds - actual)^2))
    # Update the best.predictions if the MSE is lower then the prior 
      # best
    if (debug) {
      print(MSE.validation)
      print(combo)
    }
    
    if (is.na(best.MSE) || MSE.validation < best.MSE) {
      best.MSE <- MSE.validation
      best.predictions <- combo
    }
  }
  
  # Return best.MSE and best.predictions
  return(list(Best.MSE = best.MSE, Best.Combination = best.predictions))
}

model.combos(lm, "area", c("temp","RH","wind","rain"), train, val, TRUE, FALSE)

#lm(y ~ x1 + x2)
#lm(y ~ x1 + x4 + x3)
#lm(y ~ x1 + x2 + x3 + x4)
```


```{r}
# Run the function above multiple times to showcase based on different train/ test splits, the output combination of models is different (ie. the predictor variables used may not be relevant (according to this model) to the response)

library(hash)

best.combinations.all <- hash()

numIters <- 100
numCombosMadeAsBest <- 0

preds <- c("temp","RH","wind","rain", "area")

for (i in 1:numIters) {
  training_ind <- sample(1:517, floor(.8*517), replace = FALSE)

  train <- fire2[training_ind, preds]
  val <- fire2[-training_ind, preds]
  res <- model.combos(lm, "area", c("temp","RH","wind","rain"), train, val, TRUE, FALSE)
  
  combo <- paste(res$Best.Combination, collapse=" ")
  if (is.null(best.combinations.all[[combo]])) {
    best.combinations.all[[combo]] <- TRUE
    numCombosMadeAsBest = numCombosMadeAsBest + 1
  }
}

print(paste("There are", numCombosMadeAsBest, "combos of predictors that were selected as the best out of", 2^(length(preds)-1), "possible combinations"))
```



# Chapter 3 Summary:
## Section 3.1:

## Section 3.2:

Simple linear regression is useful if there is only one predictor:
$$
Y = \beta_{0} + \beta_{1} X
$$
If we had more than one predictors, we may need to use multiple linear regression for prediction:
$$
Y = \beta_{0} + \beta_{1} X_{1} + \beta_{2} X_{2} + \dots + \beta_{p} X_{p}
$$
### {3.2.1 Estimating the Regression Coefficients}
How do we estimate the coefficients of multiple linear regression? Similar to simple linear regression, we has below formula to make prediction:
$$
\hat{y} = \hat{\beta_{0}} + \hat{\beta_{1}} x_{1} + \hat{\beta_{2}} x_{2} + \dots + \hat{\beta_{p}} x_{p}
$$
To estimate the parameter of $\beta_{0}, \beta_{1}, \dots, \beta_{p}$, we use the same least squares approach for simple linear regression:
$$
RSS 
=\sum_{i=1}^{n} (y_{i} - \hat{y_{i}})^2 
\\=\sum_{i=1}^{n} (y_{i} - \hat{\beta_{0}} - \hat{\beta_{1}} x_{i1} - \hat{\beta_{2}} x_{i2} - \dots - \hat{\beta_{p}} x_{ip})^2
$$
### {3.2.2 Some Important Questions}
Answering below questions will help us to better understand multiple linear regression:
\
1. Is at least one of the predictors useful in predicting the response?
This raise a new question: Is There a Relationship Between the Response and Predictors?
We perform a hypothesis test to find out the relationship between the response and predictors:
$$
H_{0}: \beta_{1} = \beta_{2} = \dots  = \beta_{p} = 0 \\
H_{1}: \text{at least one } \beta_{i} \text{ is nonzero}
$$
This test is performed by computing the F-statistic, which allow us to determine the p-value. If there is no relationship between the response and predictors, we may expect the F-statistic to be close to 1.If there is a relationship, we may expect the F-statistic to be greater than 1. 
$$
    F =
\\ \frac{(TSS-RSS)/p}{RSS/(n-p-1)}
$$
p-value would be more general than F-statistic for determine the relation between the response and predictors. To reject the null hypothesis, the p-values need to be less than 0.05 typically. 

\
2. Do all of the predictors help explain Y, or only a few of them?
This raise a new question: how to decide on Important Variables?
After computing the F-statistic and to examine the associated p-value, we use variable selection to determine the significance of predictors. There are three approaches for variable selection, which are forward selection, backward selection, and mixed selection. Forward selection begins with a model that contains no variable, then we start adding the most significant variables one after the other, at last, we will stop selection if it reach a point or until all the variables are selected. Backward selection begins with a model that contains all variables, then we start removing the least significant variables one after the other, at last, we will stop selection if it reach a point or until no variable is left. Mixed selection is a combination of forward and backward selection. 

\
3. How well does the model fit the data?
This raise a new question: Is the Model Fit?
Similar to the simple regression setting, we calculate RSE and $R^2$ to determine how well the model fits the data.
$$
    RSE =
\\ \sqrt{\frac{RSS}{(n-p-1)}}
$$

\
4. How accurate is our prediction?
This raise a new question: How to assess prediction accuracy?
We use confidence intervals to determine the error between $\hat{Y}$ and $f(X)$. And we use prediction intervals to the utmost extent reduce both reducible and irreducible error.

## Section 3.3:

To incorporate qualitative predictors of only two levels, we can use an indicator or dummy variable that takes on two possible numerical values e.g. 0 and 1. For example, consider a regression model based only on one 2 level qualitative predictor. We have
$$
x_i = 
\begin{cases} 1 &\text{if } i \text{th person owns a house} \\
0 &\text{if } i\text{th person does not own a house},
\end{cases}
$$
and therefore, the model is given by
$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i = \begin{cases} \beta_0 + \beta_1 + \epsilon_i &\text{if } i \text{th person owns a house} \\
\beta_0 + \epsilon_i &\text{if } i\text{th person does not own a house},
\end{cases}.
$$
This is also known as one-hot encoding. The encoding is arbitrary, and we could also flip the 0 and 1 encoding or use a 1 and -1 encoding. The only difference is the interpretation of the coefficients. 
When a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values, so we create additional dummy variables until there are 1 fewer dummy variables than the total amount of factors. The level with no dummy variable is known as the baseline. Again, the baseline level is chosen arbitrarily, and the final predictions will be the same regardless of the choice of baseline. The coefficients and their p-values do depend on the choice of coding, however. We can use and F-test to test $H_0: \beta_1 = \beta_2 = 0$, which does not depend on the coding. 

We can use a dummy variable approach even when incorporating both quantitative and qualitative predictors. 

The standard linear regression model provides interpretable results, but makes several highly restrictive assumptions that are often violated in practice. Two of the most important are that the relationship between predictors and response are additive and linear, i.e. that the association between predictor $X_j$ and the response $Y$ does not depend on the values of the other predictors, and that the change in the response $Y$ associated with a one-unit change in $X_j$ is constant, regardless of the value of $X_j$. 

One way to extend the standard linear regression model (with two variables for example) is to include a third predictor, called an interaction term, which is constructed by computing the product of $X_1$ and $X_2$, which results in the model
$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1X_2 + \epsilon.
$$
Note that this can be rewritten as
$$
Y = \beta_0 + \tilde{\beta}_1X_1 + \beta_2 X_2 + \epsilon,
$$
where $\tilde{\beta}_1 = \beta_1 + \beta_3X_2$. Hence the association between $X_1$ and $Y$ is no longer constant. 

The hierarchical principle sates that if we include an interaction in a model, we should also include the main effects, even if the p-values associated with their coefficients are not significant. $X_1 \times X_2$ is typically correlated with $X_1$ and $X_2$, so leaving them out tends to alter the meaning of the interaction. 

In some cases, the true relationship between the response and the predictors may be non-linear. A simple way to directly extend the linear model to accommodate non-linear relationships is to use polynomial regression. A simple approach for incorporating non-linear associations in a linear model is to include transformed versions of the predictors, i.e. 
$$
Y = \beta_0 + \beta_1X_1 + \beta_2X_2^2 + \epsilon.
$$
This is still a linear model with respect to the $\beta$ parameters. This approach is known as polynomial regression since we have included polynomial functions of the predictors in the regression model.

There are a few common problems that occur when fitting a linear regression model:

1. Non-linearity of the response-predictor relationships

    The linear regression model assumes a straight-line relationship between the predictors and the response. If the true relationship isn't linear, then the conclusions drawn from a linear model are suspect, and the prediction accuracy can be poor. Residual plots are useful visual tools to identify non-linearity. We can plot the residuals $e_i = y_i -\hat{y}_i$, versus the predictor $x_i$ or in the multiple regression case, we instead plot the residuals versus the predicted values $\hat{y}_i$. Ideally, the residual plot will show no pattern. If it does, it may indicate a problem with the linear model. If the residual plot indicates a non-linear association, a simple approach is to use non-linear transformations of the predictors, such as $\log X, \sqrt{X},$ and $X^2$ in the model. 

2. Correlation of error terms

    The linear regression model assumes the error terms $\epsilon_1, \epsilon_2, \dots, \epsilon_n$ are uncorrelated since if there is a correlation, the estimated standard errors will tend to underestimate the true standard errors. As a result, confidence and prediction intervals and will be narrower than they should be and p-values may be lower than they should be. 
    
    Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. Observations that are obtained at adjacent time points will have positively correlated errors in many cases. To determine this, we can use a residual plot as a function of time. If the errors are uncorrelated, there should be no pattern, but if they are positively correlated, there may be tracking in the residuals, i.e. adjacent residuals may have similar values. 
    
    Correlation among the error terms can also occur outside of time series data. The assumption of uncorrelated errors is extremely important for linear regression and for other statistical methods. Good experimental design is crucial to mitigate the risk. 
    
3. Non-constant variance of error terms
    
    Standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon the assumption of constant variance, $Var(\epsilon_i) = \sigma^2$. It's often the case that the variances of the error terms are non-constant. One can identify non-constant variances in the errors from the presence of a funnel shape in the residual plot. One possible solution is to transform the response $Y$ using a concave function such as $\log Y$ or $\sqrt{Y}$ which leads to a greater amount of shrinkage of the larger responses, and hence a reduction in heteroscedasticity. If we have a good idea of the variance of each response, we can fit our model by weighted least squares with weights proportional to the inverse variances. 
    
4. Outliers

    An outlier is a point for which $y_i$ is far from the value predicted by the model. Residual plots can be used to identify outliers, but it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. To address this, we can plot the studentized residuals, computed by dividing each residual $e_i$ by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute value are possible outliers. One solution is to simply remove the observation, but an outlier may also indicate a deficiency of the model. 
    
5. High-leverage points
    
    Observations with high leverage have an unusual value for $x_i$. High leverage observations tend to have a sizable impact on the estimated regression line. Any problems with high leverage points may invalidate the entire fit, so it is important to identify high leverage observations. 
    
    In a multiple linear regression, it is possible to have an observation that is well within the range of each individual predictor's values, but is unusual in terms of the full set of predictors. In order to quantify an observation's leverage, we compute the leverage statistic. For a simple linear regression, it is
    $$
    h_i = \frac{1}{n} + \frac{(x_i+\bar{x})^2}{\sum_{j=1}^n(x_{j}-\bar{x})^2}.
    $$
    The leverage statistic is always between $1/n$ and 1, and the average leverage for all the observations is always equal to (p+1)/n, so if an observation has a leverage statistic that greatly exceeds this, it may be a high leverage point.
    
6. Collinearity

    Collinearity refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. 
    
    Since collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\hat{\beta}_j$ to grow. As a result, if there is collinearity, we may fail to reject $H_0:\beta_j=0$. The power of the hypothesis test (the probability of correctly detecting a non-zero coefficient) is reduced by collinearity. 
    
    A simple way to detect collinearity is to look at the correlation matrix of the predictors. If there is an element that is large in absolute value, that indicates a pair of highly correlated variables. It is possible for collinearity to exist between three of more variables even if no pair of variables has a high correlation. This is called multicollinearity. A better way to assess multicollinearity is to compute the variance inflaction factor (VIF),
  $$
  VIF(\hat{\beta}_j) = \frac{1}{1-R^2_{X_j|X_{-j}}},
  $$
    where $R^2_{X_j|X_{-j}}$ is the $R^2$ from a regression of $X_j$ onto all of the other predictors. The VIF is the ratio of the variance of $\hat\beta_j$ when fitting the full model divided by the variance of $\hat\beta_j$ if fit on its own. As a rule of thumb, a value that exceeds 5 or 10 indicates a high amount of collinearity. 

    There are two simple solutions, drop one of the problematic variables from the regression, or combine the collinear variables together into a single predictor. 

## Section 3.4:

<ol>
  <li><b>Exploring the relationship between sales and advertising budget:</b> Since there are three variables that make up advertising budget (TV, radio, and newspaper), a multiple linear regression model is used with TV, radio and newspaper as the predictors and sales as the response. An F-statistic can be used to test if $H_0: \beta_{TV} = \beta_{radio} = \beta_{newspaper} = 0$. For this dataset, the pvalue was low providing significant evidence of the relationship between sales and advertising budget.</li>
  <li><b>Strength of the relationship:</b>RSE which estimates the standard deviation of the response (in this case the sales) between the population regression line; and, $R^2$ which is the percentage of variability of the response explained by the predictors are two good measures for the strength of a relationship between response and predictors. </li>
  <li><b>Which media are association with sales?:</b> Individual pvalues of the predictors can be viewed to identify the significance of any individual predictor to the response.</li>
  <li><b>How large is the association between each medium and sales?:</b> Constructing confidence intervals for the individual $\hat{\beta}$ coefficients is a good way to go about measure the size of association between the predictors and the response. If the confidence interval includes 0, then it may suggest a predictor has non- significant impact on the response. However, collinearity could be an issue which results in large confidence bounds. VIF scores can be viewed to ensure collinearity is not an issue. Another technique to identify individual relationships between the predictors and response is to perform simple linear regression models between each predictor and response and identify size of association. </li>
  <li><b>How accurately can we predict future sales?:</b> If we are interested in an individual response then we can use a prediction interval, but if interested in an average response we can use a confidence interval. The bounds for prediction intervals are always wider then confidence intervals due to taking into account irreducible error.</li>
  <li><b>Is the relationship linear?:</b> A residual diagnostic plot can be used to identify linearity. We expect no pattern (centered at 0) among the residuals if the relationship is linear. Transformations can be used if patterns are found.</li>
  <li><b>Is there synergy among the advertising media?:</b>Interactions among variables should be explored.</li>
</ol>

## Section 3.5: Comparison of Linear Regression with K-Nearest Neighbors

Linear regression is a parametric method due to its linearity assumption. The advantage to parametric methods is they are easy to fit because of the small number of parameters to estimate, tests for significance are easy to perform, and interpretation is usually clear. The disadvantage of parametric methods are their strong assumptions. If the true model deviates from our assumed model, then the results will be poor. 

Non- parametric methods do not have assumptions of the model form and are generally more flexible then parametric methods. 

### KNN Regression:
Is very similar to KNN classifier (but for when the response is continuous) in that the model will identify the K closest points to a point of prediction (denoted as $N_0$) and averages those to form the prediction. 

$$\hat{f}(x_0) = \frac{1}{K}\sum_{x_i \in N_0} y_i$$
The best value of K is based on the bias- variance trade off. Smaller K will have lower bias but higher variance whereas larger values of K will have higher bias but lower variance. KNN can suffer from the curse of dimensionality where in higher dimensions, the K observations closest to the point of prediction may be very off.

In the choice between parametric or non- parametric methods, if the true form of the relationship matches the parametric models assumptions, then it will outperform the non- parametric method. 

"In general, parametric methods will tend to outperform non- parametric approaches when there is a small number of observations per predictor".

\
## Problem 3.9
\
(a) Produce a scatterplot matrix which includes all of the variables in the data set
```{r}
library(ISLR)
pairs(Auto)
```
\
(b) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.
```{r}
cor(subset(Auto, select = -name))
```
\
(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results.
```{r}
mlr = lm(mpg ~ . - name, data = Auto)
summary(mlr)
```
\
Both F-statistic and p-value's value provide evidence against the null hypothesis. As a result, we can conclud there is a relationship between the predictors and the response. 
Based on the p-value, displacement, weight, year, and origin have a statistically significant relationship to the response.
The estimate coefficient for year is 0.750773, which indicate the mpg goes up 0.750773 per year. 

\
(d) Use the plot() function to produce diagnostic plots of the linear regression fit. 
\
By the residual plot, we can conclude the data is the model is not fit well since there is non-linear pattern showing. Also, scale-location plot show there is no point located outside of range [-2,2], which indicates there may be no outliers. Point 14 is the high leverage point. 
```{r}
par(mfrow = c(2, 2))
plot(mlr)
```
\
(e) Use the * and : symbols to fit linear regression models with interaction effects. 
\
Cylinders * weight is statistically significant, whereas displacement * acceleration is not. 
```{r}
mlr.new <-  lm(mpg ~ displacement * acceleration + cylinders * weight, data = Auto)
summary(mlr.new)
```
\
(f) Try a few different transformations of the variables.
\
The log(cylinders) transformation is more significant than cylinders, but the sqrt(acceleration) is less significant than acceleration. We assume that is depend on the variables selection. 
```{r}
summary(lm(mpg ~ . - name + log(cylinders), data=Auto))
summary(lm(mpg ~ . - name + sqrt(acceleration), data=Auto))
```
