---
title: "Homework 4"
author: "Brandon Amaral, Monte Davityan, Nicholas Lombardo, Hongkai Lu"
date: '2022-10-03'
output: pdf_document
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
options(knitr.kable.NA = '')
set.seed(123)

library(ISLR2)
library(keras)
library(tidyverse)
library(glmnet)
```

## 1 Murphy Chapter 13 & ISLR Chapter 10 Summaries

### Section 10.6 When to Use Deep Learning
The text shows three way to determine when to use deep learning:
1: Occam’s razor principle: when faced with several methods that give roughly equivalent performance, pick the simplest.
2: According to the text, "if we can produce models with the simpler tools that perform as well, they are likely to be easier to fit and understand, and potentially less fragile than the more complex approaches."
3: When the sample size of the data is extremely large and the interpretability of the model is not our main priority, we should use Deep Learning to be our first choice.  

### Section 10.7 Fitting a Neural Network

#### 10.7.1 Backpropagation

#### 10.7.2 Regularization and Stochastic Gradient Descent

#### 10.7.3 Dropout Learning

#### 10.7.4 Network Tuning

### Section 10.8 Interpolation and Double Descent
Double descent is a phenomenon that the test error has a U-shape before the interpolation threshold is reached, then it descends again as an increasingly flexible model is fit.That may applied to deep learning as well. However, even though double descent can occasionally occur in neural networks model, we do not rely on this behavior typically; meanwhile, it is important to know that the bias-variance trade-oﬀ always holds.

## 2 Reproduction of ISLR Lab 10.9
### 10.9.1 A Single Layer Network on the Hitters Data
First, We load the Hitters data and split it to training and test set.
```{r}
Gitters <- na.omit(Hitters)
n <- nrow(Gitters)
set.seed(13)
ntest <- trunc(n/3)
testid <- sample(1:n, ntest)
```
Then, we fit a linear model and obtain the mean absolute prediction error, which is 254.6687.
```{r}
lfit <- lm(Salary ~., data=Gitters[-testid,])
lpred <- predict(lfit, Gitters[testid,])
with(Gitters[testid,], mean(abs(lpred - Salary)))
```
Next, we fit a lasso model. Similarly, we calculate the absolute prediction error of the lasso model, which is slightly less than the result of linear model.
```{r}
library(glmnet)
x <- scale(model.matrix(Salary ~. -1, data=Gitters))
y <- Gitters$Salary

cvfit <- cv.glmnet(x[-testid, ], y[-testid], type.measure='mae')
cpred <- predict(cvfit, x[testid,], s='lambda.min')
mean(abs(y[testid] - cpred))
```

Last, we fit the neural network with library keras. As the text written, it is a challenge to get keras running on computer. The error I got is "CondaSSLError: Encountered an SSL error. Most likely a certificate verification issue." Still figure out a way to get this installing. 
```{r}
modnn <- keras_model_sequential() %>% 
  layer_dense(units=50, activation='relu', input_shape = ncol(x)) %>% 
  layer_dropout(rate=0.4) %>%
  layer_dense(units=1)
```

```{r, cache=TRUE}
modnn %>% compile(loss='mse', optimizer=optimizer_rmsprop(), metrics=list('mean_absolute_error'))
history <- modnn %>% fit(
  x[-testid, ], y[-testid], epochs=200, batch_size=32, 
  validation_data=list(x[testid,], y[testid])
)
plot(history, smooth = FALSE)
```


```{r}
npred <- predict(modnn, x[testid,])
mean(abs(y[testid] - npred))
```
### 10.9.2 A Multilayer Network on the MNIST Digit Data
```{r}
mnist <- dataset_mnist()
x_train <- mnist$train$x
g_train <- mnist$train$y
x_test <- mnist$test$x
g_test <- mnist$test$y
dim(x_train)
```

```{r}
x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
y_train <- to_categorical(g_train, 10)
y_test <- to_categorical(g_test, 10)
```

```{r}
x_train <- x_train/255
x_test <- x_test/255
```

```{r}
modelnn <- keras_model_sequential()
modelnn %>% 
  layer_dense(units=256, activation="relu", input_shape=c(784)) %>%
  layer_dropout(rate=0.4) %>%
  layer_dense(units=128, activation='relu') %>%
  layer_dropout(rate=0.3) %>%
  layer_dense(units=10, activation='softmax')

#summary(modelnn)
```

```{r}
modelnn %>% compile(loss='categorical_crossentropy', optimizer=optimizer_rmsprop(), metrics=c("accuracy"))
```

```{r, cache=TRUE}
system.time(
  history <- modelnn %>% 
    fit(x_train, y_train, epochs=30, batch_size=128, validation_split=0.2)
)
```

```{r}
accuracy <- function(pred, truth) {
   mean(drop(as.numeric(pred)) == drop(truth)) }
modelnn %>% predict(x_test) %>% k_argmax() %>% accuracy(g_test)
```

```{r}
modellr <- keras_model_sequential() %>%
   layer_dense(input_shape = 784, units = 10,
       activation = "softmax")
#summary(modellr)
```

```{r, cache=TRUE}
modellr %>% compile(loss = "categorical_crossentropy",
     optimizer = optimizer_rmsprop(), metrics = c("accuracy"))
modellr %>% fit(x_train, y_train, epochs = 30,
      batch_size = 128, validation_split = 0.2)
modellr %>% predict(x_test) %>% k_argmax() %>% accuracy(g_test)
```

### 10.9.3 Convolutional Neural Networks
```{r, cache=TRUE}
cifar100 <- dataset_cifar100()
names(cifar100)
x_train <- cifar100$train$x
g_train <- cifar100$train$y
x_test <- cifar100$test$x
g_test <- cifar100$test$y
dim(x_train)
range(x_train[1,,, 1])
```

```{r}
x_train <- x_train / 255
x_test <- x_test / 255
y_train <- to_categorical(g_train, 100)
dim(y_train)
```

```{r}
library(jpeg)
par(mar = c(0, 0, 0, 0), mfrow = c(5, 5))
index <- sample(seq(50000), 25)
for (i in index) plot(as.raster(x_train[i,,, ]))
```

```{r}
model <- keras_model_sequential() %>%
   layer_conv_2d(filters = 32, kernel_size = c(3, 3),
      padding = "same", activation = "relu",
      input_shape = c(32, 32, 3)) %>%
   layer_max_pooling_2d(pool_size = c(2, 2)) %>%
   layer_conv_2d(filters = 64, kernel_size = c(3, 3),
      padding = "same", activation = "relu") %>%
   layer_max_pooling_2d(pool_size = c(2, 2)) %>%
   layer_conv_2d(filters = 128, kernel_size = c(3, 3),
      padding = "same", activation = "relu") %>%
   layer_max_pooling_2d(pool_size = c(2, 2)) %>%
   layer_conv_2d(filters = 256, kernel_size = c(3, 3),
      padding = "same", activation = "relu") %>%
   layer_max_pooling_2d(pool_size = c(2, 2)) %>%
   layer_flatten() %>%
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 512, activation = "relu") %>%
   layer_dense(units = 100, activation = "softmax")
#summary(model)
```

```{r, cache=TRUE}
model %>% compile(loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(), metrics = c("accuracy"))
#history <- model %>% fit(x_train, y_train, epochs = 30,
history <- model %>% fit(x_train, y_train, epochs = 10,
    batch_size = 128, validation_split = 0.2)
model %>% predict(x_test) %>% k_argmax() %>% accuracy(g_test)
```

### 10.9.4 Using Pretrained CNN Models
```{r}
img_dir <- "book_images"
image_names <- list.files(img_dir)
num_images <- length(image_names)
x <- array(dim = c(num_images, 224, 224, 3))
for (i in 1:num_images) {
   img_path <- paste(img_dir, image_names[i], sep = "/")
   img <- image_load(img_path, target_size = c(224, 224))
   x[i,,, ] <- image_to_array(img)
}
x <- imagenet_preprocess_input(x)
```

```{r, cache=TRUE}
model <- application_resnet50(weights = "imagenet")
#summary(model)
```

```{r}
pred6 <- model %>% predict(x) %>%
   imagenet_decode_predictions(top = 3)
names(pred6) <- image_names
print(pred6)
```

### 10.9.5 IMDb Document Classification
```{r}
max_features <- 10000
imdb <- dataset_imdb(num_words = max_features)
c(c(x_train, y_train), c(x_test, y_test)) %<-% imdb
```

```{r}
x_train[[1]][1:12]
```

```{r}
word_index <- dataset_imdb_word_index()
decode_review <- function(text, word_index) {
   word <- names(word_index)
   idx <- unlist(word_index, use.names = FALSE)
   word <- c("<PAD>", "<START>", "<UNK>", "<UNUSED>", word)
   idx <- c(0:3, idx + 3)
   words <- word[match(text, idx, 2)]
   paste(words, collapse = " ")
}
decode_review(x_train[[1]][1:12], word_index)
```

```{r}
library(Matrix)
one_hot <- function(sequences, dimension) {
   seqlen <- sapply(sequences, length)
   n <- length(seqlen)
   rowind <- rep(1:n, seqlen)
   colind <- unlist(sequences)
   sparseMatrix(i = rowind, j = colind,
      dims = c(n, dimension))
}
```

```{r}
x_train_1h <- one_hot(x_train, 10000)
x_test_1h <- one_hot(x_test, 10000)
dim(x_train_1h)
nnzero(x_train_1h) / (25000 * 10000)
```

```{r}
set.seed(3)
ival <- sample(seq(along = y_train), 2000)
```

```{r}
library(glmnet)
fitlm <- glmnet(x_train_1h[-ival, ], y_train[-ival],
    family = "binomial", standardize = FALSE)
classlmv <- predict(fitlm, x_train_1h[ival, ]) > 0
acclmv <- apply(classlmv, 2, accuracy,  y_train[ival] > 0)
```

```{r}
par(mar = c(4, 4, 4, 4), mfrow = c(1, 1))
plot(-log(fitlm$lambda), acclmv)
```

```{r, cache=TRUE}
model <- keras_model_sequential() %>%
   layer_dense(units = 16, activation = "relu",
      input_shape = c(10000)) %>%
   layer_dense(units = 16, activation = "relu") %>%
   layer_dense(units = 1, activation = "sigmoid")
model %>% compile(optimizer = "rmsprop",
   loss = "binary_crossentropy", metrics = c("accuracy"))
history <- model %>% fit(x_train_1h[-ival, ], y_train[-ival],
   epochs = 20, batch_size = 512,
   validation_data = list(x_train_1h[ival, ], y_train[ival]))
```

```{r, cache=TRUE}
history <- model %>% fit(
    x_train_1h[-ival, ], y_train[-ival], epochs = 20,
    batch_size = 512, validation_data = list(x_test_1h, y_test)
    )
```

### 10.9.6 Recurrent Neural Networks

#### Sequential Models for Document Classification

```{r}
wc <- sapply(x_train, length)
median(wc)
sum(wc <= 500) / length(wc)
```

```{r}
maxlen <- 500
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
dim(x_train)
dim(x_test)
x_train[1, 490:500]
```

```{r}
model <- keras_model_sequential() %>%
   layer_embedding(input_dim = 10000, output_dim = 32) %>%
   layer_lstm(units = 32) %>%
   layer_dense(units = 1, activation = "sigmoid")
```

```{r, cache=TRUE}
model %>% compile(optimizer = "rmsprop",
    loss = "binary_crossentropy", metrics = c("acc"))
#history <- model %>% fit(x_train, y_train, epochs = 10,
history <- model %>% fit(x_train, y_train, epochs = 3,
    batch_size = 128, validation_data = list(x_test, y_test))
plot(history, smooth = FALSE)
```

#### Time Series Prediction

```{r}
xdata <- data.matrix(
 NYSE[, c("DJ_return", "log_volume","log_volatility")]
 )
istrain <- NYSE[, "train"]
xdata <- scale(xdata)
```

```{r}
lagm <- function(x, k = 1) {
   n <- nrow(x)
   pad <- matrix(NA, k, ncol(x))
   rbind(pad, x[1:(n - k), ])
}
```

```{r}
arframe <- data.frame(log_volume = xdata[, "log_volume"],
   L1 = lagm(xdata, 1), L2 = lagm(xdata, 2),
   L3 = lagm(xdata, 3), L4 = lagm(xdata, 4),
   L5 = lagm(xdata, 5)
 )
```

```{r}
arframe <- arframe[-(1:5), ]
istrain <- istrain[-(1:5)]
```

```{r}
arfit <- lm(log_volume ~ ., data = arframe[istrain, ])
arpred <- predict(arfit, arframe[!istrain, ])
V0 <- var(arframe[!istrain, "log_volume"])
1 - mean((arpred - arframe[!istrain, "log_volume"])^2) / V0
```

```{r}
arframed <-
    data.frame(day = NYSE[-(1:5), "day_of_week"], arframe)
arfitd <- lm(log_volume ~ ., data = arframed[istrain, ])
arpredd <- predict(arfitd, arframed[!istrain, ])
1 - mean((arpredd - arframe[!istrain, "log_volume"])^2) / V0
```

```{r}
n <- nrow(arframe)
xrnn <- data.matrix(arframe[, -1])
xrnn <- array(xrnn, c(n, 3, 5))
xrnn <- xrnn[,, 5:1]
xrnn <- aperm(xrnn, c(1, 3, 2))
dim(xrnn)
```

```{r}
model <- keras_model_sequential() %>%
   layer_simple_rnn(units = 12,
      input_shape = list(5, 3),
      dropout = 0.1, recurrent_dropout = 0.1) %>%
   layer_dense(units = 1)
model %>% compile(optimizer = optimizer_rmsprop(),
    loss = "mse")
```

```{r, cache=TRUE}
history <- model %>% fit(
    xrnn[istrain,, ], arframe[istrain, "log_volume"],
#    batch_size = 64, epochs = 200,
    batch_size = 64, epochs = 75,
    validation_data =
      list(xrnn[!istrain,, ], arframe[!istrain, "log_volume"])
  )
kpred <- predict(model, xrnn[!istrain,, ])
1 - mean((kpred - arframe[!istrain, "log_volume"])^2) / V0
```

```{r}
model <- keras_model_sequential() %>%
   layer_flatten(input_shape = c(5, 3)) %>%
   layer_dense(units = 1)
```

```{r}
x <- model.matrix(log_volume ~ . - 1, data = arframed)
colnames(x)
```

```{r, cache=TRUE}
arnnd <- keras_model_sequential() %>%
   layer_dense(units = 32, activation = 'relu',
      input_shape = ncol(x)) %>%
   layer_dropout(rate = 0.5) %>%
   layer_dense(units = 1)
arnnd %>% compile(loss = "mse",
    optimizer = optimizer_rmsprop())
history <- arnnd %>% fit(
#    x[istrain, ], arframe[istrain, "log_volume"], epochs = 100, 
    x[istrain, ], arframe[istrain, "log_volume"], epochs = 30, 
    batch_size = 32, validation_data =
      list(x[!istrain, ], arframe[!istrain, "log_volume"])
  )
plot(history, smooth = FALSE)

npred <- predict(arnnd, x[!istrain, ])
1 - mean((arframe[!istrain, "log_volume"] - npred)^2) / V0
```


## 3 ISLR Problems

### 10.7
We fit a neural network with a single hidden layer and 10 units to the `Default` data, and compare its performance to a logistic regression model. First, we scale our data in a model matrix, excluding the usual first column of 1's for an intercept term, and create a separate response vector of 1's and 0's. 
```{r}
x <- model.matrix(default ~ . -1, data = Default) |> scale()
y <- ifelse(Default$default == "Yes",1,0)
```

Next, we partition our data into training and test datasets, training on a random set of 80% of the data, and testing on the remaining 20%. 

```{r}
n <- nrow(x)
test_ind <- sample(1:n, n/5)

x_train <- x[-test_ind,]
x_test <- x[test_ind,]
y_train <- y[-test_ind]
y_test <- y[test_ind]
```

Now, we build the model architecture, defining a single layer model with 10 units in the hidden layer, with the usual ReLU activation function and a dropout rate of 20% for regularization. Since we are predicting a binary response, we use the sigmoid activation function in the output layer. Then, we compile the model, using the binary cross entropy loss function as a measure of fit. 
```{r}
nnmodel <- keras_model_sequential() |>
  layer_dense(units = 10, activation = "relu", input_shape = ncol(x)) |>
  layer_dropout(rate = 0.20) |>
  layer_dense(units = 1, activation = "sigmoid")

nnmodel |> compile(
  loss = "binary_crossentropy", 
  optimizer = optimizer_rmsprop(), 
  metrics = "accuracy")
```


We fit the model using a batch size of 30 with 40 epochs. We also further split the data into a third validation set, again with 20%, so we fit the model on 6400 training observations. Then, each epoch is $6400/30\approx 214$ stochastic gradient descent steps. We plot the loss of the model alongside its
```{r, cache=TRUE}
history <- nnmodel |>
  fit(x_train, y_train, epochs = 40, batch_size = 30, validation_split = 0.2)

plot(history, smooth = FALSE)
```

Now, we make our predictions on the test set and find the model's misclassification rate.
```{r}
y_test_pred <- nnmodel |> predict(x_test)
NN_misclass_rate <- 1/length(y_test)*sum(y_test != round(y_test_pred))
```

Now, we compare this with a logistic regression model.
```{r}
log_model <- glm(y_train ~ x_train, family = binomial(link = logit))
y_log_test_pred <- round(predict(log_model, newdata = data.frame(x_test), type = "response"))
logistic_misclass_rate <- 1/length(y_test)*sum(y_test != round(y_log_test_pred))
```
The table shows the misclassification rates for both models
```{r}
misclass <- data.frame(Logistic = logistic_misclass_rate, NeuralNet = NN_misclass_rate)
knitr::kable(misclass)
```
In the table, we see that the logistic regression model performed considerably worse, with a misclassification rate of about $`r round(logistic_misclass_rate,4)*100`$% versus the neural net's misclassification rate of about $`r round(NN_misclass_rate,4)*100`$%.



### 10.8

```{r}
img_dir <- "MyAnimalPics"
image_names <- list.files (img_dir)
num_images <- length (image_names)
x <- array ( dim = c(num_images , 224, 224, 3))
for (i in 1:num_images) {
  img_path <- paste (img_dir , image_names[i], sep = "/")
  img <- image_load (img_path, target_size = c(224, 224))
  x[i,,, ] <- image_to_array(img)
}
x <- imagenet_preprocess_input (x)
```


```{r}
model <- application_resnet50(weights = "imagenet")
#summary (model)
```

```{r}
pred6 <- model %>% predict (x) %>%
  imagenet_decode_predictions (top = 5)
names (pred6) <- image_names
print (pred6)
```


Interestingly, the model does decently well for semi- obvious, clear pictures such as the Stingray, monkey, and dog images. However, it does really poorly when the animal is not obvious such as the coyote, parrot, and fish.
