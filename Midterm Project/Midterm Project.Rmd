---
title: "Midterm Project"
author: "Brandon Amaral, Monte Davityan, Nicholas Lombardo, Hongkai Lu"
date: "10/18/2022"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
options(knitr.kable.NA = '')
```

## 1 ISLR Chapter 9 Summary

### 9.1 Maximal Margin Classifier 

This section is focus on definition and introduction the concept of an optimal separating hyperplane.

#### 9.1.1 What Is a Hyperplane?
\
A line is a hyperplane in a two dimensions space; a plane is a hyperplane in a three dimensions space; a flat affine subspace of p-1 dimension subspace is a hyperplane in a p-dimensional space. 
The mathematical definition of a hyperplane is defined by the equation as follow:
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}=0
$$
We have below sense for a p-dimensional hyperplane.

1: A point X lies on the hyperplane if $ X = (X_{1},X_{2},\dots,X_{p})^T$ in p-dimensional space sattisfies.

2: A point X lies to one side of the hyperplane if 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}>0
$$

3: A point X lies to other side of the hyperplane if 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}<0
$$
Thus, we can think of the hyperplane as dividing p-dimensional space into two halves.

#### 9.1.2 Classification Using a Separating Hyperplane
\
First, suppose that we have a n X p data matrix $\mathbf X$ that consists of n training observations in a p-dimensinal space. The training observations fall into two groups, where $ y_{1}, \dots, y_{n} \in {-1,1} $ and −1 represents one class and 1 the other class. 
Second, try to constuct a separating hyperplane which is a hyperplane that separates the training observations perfectly according to their class labels. That separating hyperplane has below property:

1: 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}>0 \quad if \quad y_{i}=1,
$$
2: 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}<0  \quad if \quad y_{i}=-1,
$$
Next, we use the constructed separating hyperplane to construct a very natural classifier: a test observation is assigned a class depending on which side of the hyperplane it is located. The classifier is based on a separating hyperplane leads to a linear decision boundary.


#### 9.1.3 The Maximal Margin Classifier 
\
The goal is to construct a classifier based upon a separating hyperplane. Instinctively, the maximal margin hyperplane is the separating hyperplane that optimal separating hyperplane is farthest from the training observations. Then, we can classify a test observation based on which side of the maximal margin hyperplane it lies, which is the maximal margin classifier.

#### 9.1.4 Construction of the Maximal Margin Classifier 
\
Suppose we have a set of n training observations $ x_{1}, \dots, x_{n} \in \mathbb{R}^p $ and associated class labels $ y_{1}, \dots, y_{n} \in {-1,1} $ , then the maximal margin hyperplane is to solve the optimization problem as follows:
$$
  (9.9): \quad \underset {\beta_{0},\beta_{1},\dots,\beta_{p},M} {maximize} M
$$
$$
  (9.10): \quad subject \quad to \quad \sum_{j=1}^{p}\beta_{j}^2 =1
$$
$$
  (9.11): \quad y_{i}(\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\dots+\beta_{p}x_{ip}) \geq M \quad \forall i=1,\dots,n)
$$
\
First, use the inequation (9.11) to constrain each observation liles on the correct side of the hyperplane. Second, both (9.10) and (9.11) ensure that each observation is on the correct side of the hyperplane and at least a distance M from the hyperplane. Last, M represents the margin of our hyperplane, and the
optimization problem chooses $\beta_{0},\beta_{1},\dots,\beta_{p}$ to maximize M.


#### 9.1.5 The Non-separable Case 
\
If a separating hyperplane exists, we can use the maximal margin classifier to perform classification. But in many cases, there is no separating hyperplane. The next section is focus on how to generalize the maximal margin classifier to the non-separable case, which is known as the support vector classifier.

#### 9.2.1 Overview of the Support Vector Classifier
\
Suppose we have two classes observations. Instead of perfectly separating the two classes, the support vector classifier is more interesting on the greater robustness to individual observations and better classification of most of the training observations. In other words, the support vector classifier allows some observations to be lie on the incorrect side of the margin or even the incorrect side of the hyperplane. 

#### 9.2.2 Details of the Support Vector Classifier 
\
Similar to "Construction of the Maximal Margin Classifier", then the Support Vector Classifier is to solve the optimization problem as follows:
$$
  (9.12): \quad \underset {\beta_{0},\beta_{1},\dots,\beta_{p},\epsilon_{1},\dots,\epsilon_{n},M} {maximize} M
$$
$$
  (9.13): \quad subject \; to \; \sum_{j=1}^{p}\beta_{j}^2 =1
$$
$$
  (9.14): \quad y_{i}(\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\dots+\beta_{p}x_{ip}) \geq M(1-\epsilon_{i}) \quad \forall i=1,\dots,n)
$$
$$
  (9.15): \epsilon_{i} \geq 0, \quad \sum_{i=1}^{n}\epsilon_{i}\leq C, where \; C \; is \; a \; nonnegative \; tuning \; parameter
$$
\
First, we use the slack variable $ \epsilon_{i} $ to locate the ith observation relative to the hyperplane and relative to the margin as follow:
\
(1): If $\epsilon_{i} = 0$ then the ith observation is on the correct side of the margin
\
(2): If $\epsilon_{i} \geq 0$ then the ith observation is on the wrong side of the margin
\
(3): If $\epsilon_{i} \geq 1$ then it is on the wrong side of the hyperplane.
\
\
Then, we use the C in (9.15) to be the controller as bias-variance trade-oﬀ of the statistical learning technique. According to the text, we know "When C is small, we seek narrow margins that are rarely violated; this amounts to a classifier that is highly fit to the data, which may have low bias but high variance. On the other hand, when C is larger, the margin is wider and we allow more violations to it; this amounts to fitting the data less hard and obtaining a classifier that is potentially more biased but may have lower variance."
\
\
At last, the observations that lie directly on the margin or on the wrong side of the margin for their class are support vectors, which affect and result the support vector classifier. 

### 9.3 Support Vector Machines 
#### 9.3.1 Classification with Non-Linear Decision Boundaries 

In the two-class setting, the support vector classifier is a natural approach  for classification if the boundary between the two classes is liner. For non-linear class boundaries, we could enlarge the feature space using quadratic, cubic, and even higher-order polynomial functions of the predictors to fit a support vector classifier. 
\
\
Suppose we have a set of n training observations $ x_{1}, \dots, x_{n} \in \mathbb{R}^p $ and associated class labels $ y_{1}, \dots, y_{n} \in {-1,1} $, we use 2p features to fit (9.12) - (9.15)
$$
  X_{1},X_{1}^2,X_{2},X_{2}^2,\dots,X_{p},X_{p}^2.
$$
Then, we have
$$
  (9.16): \quad \underset {\beta_{0},\beta_{11},\beta_{12},\dots,\beta_{p1},\beta_{p2},\epsilon_{1},\dots,\epsilon_{n},M} {maximize} M
$$
$$
  subject \; to \; y_{i}(\beta_{0}+\sum_{j=1}^{p}\beta_{j1}x_{ij}^2+\sum_{j=1}^{p}\beta_{j2}x_{ij}^2) \geq M(1-\epsilon_{i})
$$
$$
  \sum_{i=1}^{n}\epsilon_{i}\leq C, \quad \epsilon_{i} \geq 0, \quad \sum_{j=1}^{p}\sum_{k=1}^{2}\beta_{jk}^2=1
$$

#### 9.3.2 The Support Vector Machine 

By using kernels, the extension of the support vector classifier that results from enlarging the feature space in a specific way is known as The support vector machine (SVM). Suppose we have a support vector classifier can be represented as
$$
  f(x) = \beta_{0}+\sum_{i \in S}\alpha_{i} K\langle x, x_{i}\rangle
$$
(1): If we take a linear kernel $$ K\langle x, x_{i}\rangle = \sum_{j=1}^{p}x_{ij}x_{i'j}  $$  , it is a linear support vector classifier.
\
(2): If we take a polynomial kernel of degree d  $$  K\langle x, x_{i}\rangle = (1 + \sum_{j=1}^{p}x_{ij}x_{i'j})^d  $$  , it is a non-linear support vector classifier.
\
(3): If we take a radial kernel $$ K\langle x, x_{i}\rangle = exp(-\gamma \sum_{j=1}^{p}(x_{ij}-x_{i'j})^2)  $$  , it is a non-linear support vector classifier.

#### 9.3.3 An Application to the Heart Disease Data 

In this section, we fit LDA and SVM to the heart disease training data. By using ROC curves, we can see the SVM using a polynomial kernel of degree d = 1 is slightly superior than LDA. Also, Figure 9.10 displays ROC curves for SVMs using a radial kernel with three values of $ \gamma $, which indicates while a more flexible method will often produce lower training error rates and that does not improve the performance on test data. 

### 9.4 SVMs with More than Two Classes 

The goal is to extend SVMs to the more general case where we have some arbitrary number of classes. The text introduces two methods which are the one-versus-one and one-versus-all approaches. 

#### 9.4.1 One-Versus-One Classification

Suppose there are K > 2 classes, we would like to perform classification using a one-versus-one or all-pairs approach constructs $\binom{k}{2}$ SVMs. 
First, classify a test observation using each of the $\binom{k}{2}$ classifiers. Second, tally the number of times that the test observation is assigned to each
of the K classes. Last, assign the test observation to the class to which it was most frequently assigned in these $\binom{k}{2}$ pairwise classifications to obtain the final classification.

#### 9.4.2 One-Versus-All Classification

In the one-versus-all approach, when we fit K SVMs, we compare one of the K classes to the remaining K-1 classes each time. 

### 9.5 Relationship to Logistic Regression 

First, we have below ridge regression and the lasso function:
$$
  L(X, y, \beta) = \sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^2
$$

Then, we introduce the loss function of SVM:
$$
  L(X, y, \beta) = \sum_{i=1}^{n} max[0,1 - y_{i}(\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\dots+\beta_{p}x_{ip})]
$$

The hinge loss function of SVM is closely related to the loss function used in logistic regression. Due to the similarities, the results of logistic regression and the support vector classifier are often similar. SVMs tend to behave better than logistic regression if the classes are well separated; while as, logistic regression is often preferred if there is in more overlapping regimes.

## 2 Murphy Section 17.3 Summary
### 17.3.1 Large margin classifiers
### 17.3.2 The dual problem 
### 17.3.3 Soft margin classifiers 
### 17.3.4 The kernel trick 586
### 17.3.5 Converting SVM outputs into probabilities 
### 17.3.6 Connection with logistic regression 
### 17.3.7 Multi-class classification with SVMs 
### 17.3.8 How to choose the regularizer C 
### 17.3.9 Kernel ridge regression 
### 17.3.10 SVMs for regression  
 
## 3 ISLR Section 9.6 Lab Repo
In this lab, we demonstrate the support vector classifier and the support vector machine (SVM).

### 9.6.1 Support Vector Classifier
To start, we generate observations belonging to classes and visually inspect whether they are linearly separable.
```{r}
set.seed(1)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1,10), rep(1,10))
x[y == 1, ] <- x[y == 1, ] +1
plot(x, col = (3 - y))
```

We see they are not linearly separable, so a maximal margin classifier is not usable.  Let's set up a support vector classifier.  To do so, we need to encode the response as a factor variable.  We'll create a data fram with the response coded as a factor.  Then we use the svmfit() function from the e1071 library.
```{r}
dat <- data.frame(x = x, y = as.factor(y))
library(e1071)
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
```

Although we chose not to scale the data here, we may choose to do some in some contexts.  Let's plot the support vector classifier.
```{r}
plot(svmfit, dat)
```

We see that the support vectors are plotted as crosses while the other observations are plotted as circles.  We can determine the identities of the support vectors using:
```{r}
svmfit$index
```

If we want other information about our support vector classifier fit, the summary() command helps us:
```{r}
summary(svmfit)
```

Let's see how our fit is affected by using a smaller value of "cost", which reduces the penalty of margin violations and widens the margins.
```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 0.1, scale = FALSE)
plot(svmfit, dat)
svmfit$index
```

Now we have a greater number of support vectors (which outnumber the other observations!) and we see the margins are noticeably wider.  The svm() function does not explicitly output the coefficients of the linear decision boundary or width of the margin, so we can't verify these quickly.  We can use the tune() function of the e1071 library to perform cross-validation.  Let's do this with a range of values for the "cost" parameter.
```{r}
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
```

The lowest cross-validation error rate was obtained when cost = 0.1.  Conveniently, the best model is stored in the tune() function, accessed here:
```{r}
bestmod <- tune.out$best.model
summary(bestmod)
```

Let's use the predict() function to predict class labels for a set of test observations.  We'll starting by generating a test data set again.
```{r}
xtest <- matrix(rnorm(20 * 2), ncol = 2)
ytest <- sample(c(-1,1), 20, rep = TRUE)
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))
```

Now we will use the best model from our cross-validation to made predictions on the test observations.
```{r}
ypred <- predict(bestmod, testdat)
table(predict = ypred, truth = testdat$y)
```

Using that previously optimized value of cost (0.1), 17 of the test observations were correctly classified.  Let's try using a cost of 0.01 instead.
```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 0.01, scale = FALSE)
ypred <- predict(svmfit, testdat)
table(predict = ypred, truth = testdat$y)
```

Now there are 3 additional misclassified observations.  This is in line with our cross-validation which found a higher error rate with the cost value of 0.01 versus 0.1.

So far, we have only considered data sets where the two classes were not linearly separable.  Now let's explore a case where the classes are separable by a hyperplane.  We generate such a data set:
```{r}
x[y == 1, ] <- x[y == 1, ] + 0.5
plot(x, col = (y+5)/2, pch = 19)
```
We see that the observations are linearly separable, but just barely!  In order to produce strictly separated classes (as in a maximal margin classifier), we can set the "cost" value very high.

```{r}
dat <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 1e5)
summary(svmfit)
plot(svmfit, dat)
```

The support vector classifier made no errors on the training data, and the minimum number of support vectors (3) was used.  Since we see there are non-support vectors (graphed as circles) near the decision boundary, we know the margin was quite narrow.  Because of this, we can expect the model to perform poorly on test data.  Let's make one more model, this time with a smaller value than cost.

```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 1)
summary(svmfit)
plot(svmfit, dat)
```

Although our model now has a single misclassified training observation, we can expect it to perform better on test data because of the much wider margin and use of additional support vectors.

### 9.6.2 Support Vector Machine (SVM)
To use a non-linear kernal in our SVM, we can set the kernel type to other values, such as "polynomial" or "radial".  These non-linear kernels also require a second argument; we must specify a "degree" for polynomial" and a "gamma" for "radial".

Let's first generate a data with a non-linear class boundary.
```{r}
set.seed(1)
x <- matrix(rnorm(200 * 2), ncol = 2)
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] -2
y <- c(rep(1, 150), rep(2, 50))
dat <- data.frame(x = x, y = as.factor(y))
plot(x, col = y)
```

Our data is now randomly split with a clearly non-linear boundary.  It appears that a radial decision bounday may be a good fit since the observations of class 2 are generally flanked by observations of class 1 to the lower left and upper right.  Let's try a radial kernel with a gamma value of 1.
```{r}
train <- sample(200,100)
svmfit <- svm(y ~ ., data = dat[train, ], kernel = "radial", 
              gamma = 1, cost = 1)
plot(svmfit, dat[train, ])
```

This SVM does indeed have a decision boundary that is clearly not linear; it is shaped like a body of water centered around the "class 2" observations.  We can use the summary() function to get the classification counts, but it will unfortunately not count the number of correctly and incorrectly classified training observations for us.
```{r}
summary(svmfit)
```

Our SVM with a radial kernal used 31 support vectors, and we see roughly a half dozen misclassified training observations.  Again, we can reduce the number of training errors by increasing the value of cost, but we expect an over-fit, irregular decision boundary which would likely perform worse with test data.  Let's see what that would look like.
```{r}
svmfit <- svm(y ~ ., data = dat[train, ], kernel = "radial", 
              gamma = 1, cost = 1e5)
plot(svmfit, dat[train, ])
```

Let's again perform cross-validation using the tune() function to select the best choice training parameters.  This time, we will cross-validate across two parameters (gamma and cost) instead of one.  This will test a greater number of potential models (equal to the product of the number of each parameter)
```{r}
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat[train, ], kernel = "radial", 
                 ranges = list(
                   cost = c(0.1, 1, 10, 100, 1000),
                   gamma = c(0.5, 1, 2, 3, 4)
                 ))
summary(tune.out)
```

The best combination of parameters are cost = 1 and gamma = 0.5.  Let's view the accuracy of the test set predictions using a table.  We can use "-train" as an index to specify the test data in the dataframe by calling all the observations minus the training data.
```{r}
table(
    true = dat[-train, "y"],
    pred = predict(tune.out$best.model, newdata = dat[-train, ])
)
```

This SVM correctly classified 88 out of 100 test observations, a test accuracy rate of 88%.

### 9.6.3 ROC Curves
We can produce Receiver Operator Characteristic (ROC) curves using the ROCR package.  Here we create a function to plot an ROC curve given numerical scores and class labels for each observation.
```{r}
library(ROCR)
rocplot <- function(pred, truth, ...) {
  predob <- prediction(pred, truth)
  perf <- performance(predob, "tpr", "fpr")
  plot(perf, ...)
}
```

While support vector classifiers (and thus SVMs) output class labels for each observation, we can instead obtain fitted values for each observation.  These are the numerical scores used to obtain the class labels.  The sign of each fitted value typically decides which side of the decision boundary an observation lies; positives lie on one side, and negatives lie on the other.  We can obtain these fitted values directly using the "decision.values = TRUE" parameter when fitting with the svm() function.  Then, to plot the ROC curve where negatives correspond to class 1 and positives to class 2, we use the negative of the fitted values.
```{r}
svmfit.opt <- svm(y ~ ., data = dat[train, ],
                  kernel = "radial", gamma = 2, cost = 1, decision.values = T)
fitted1 <- attributes(predict(svmfit.opt, dat[train, ], decision.values = T)
                     )$decision.values
par(mfrow = c(1,2))
rocplot(-fitted1, dat[train, "y"], main = "Training Data")
```

Since we see the ROC curve is hugging the upper left-hand corner of the plot, the infer the SVM is producing generally accurate predictions.  If we increase the gamma value, our fit will be more flexible and hopefully more accurate.
```{r}
svmfit.flex <- svm(y ~ ., data = dat[train, ], 
                   kernel = "radial", gamma = 50, cost = 1, decision.values = T)
fitted2 <- attributes(
  predict(svmfit.flex, dat[train, ], decision.values = T))$decision.values
rocplot(-fitted1, dat[train, "y"], main = "Training Data")
rocplot(-fitted2, dat[train, "y"], col = "red", add = T)
```

An accuracy rate of 100% on training data usually doesn't bode well for test accuracy.  We have probably over-fit the training data with the greater flexibility of gamma = 50. After all, we are really most interested in the level of prediction accuracy on test data.  Let's now plot ROC curves for the test data.
```{r}
fittedtest1 <- attributes(
  predict(svmfit.opt, dat[-train, ], decision.values = T))$decision.values
rocplot(-fittedtest1, dat[-train, "y"], main = "Test Data")
fittedtest2 <- attributes(
  predict(svmfit.flex, dat[-train, ], decision.values = T))$decision.values
rocplot(-fittedtest2, dat[-train, "y"], col = "red", add = T)
```

We do indeed see that the less flexible model with gamma = 2 does outperform the overly flexible model in test prediction accuracy.

### 9.6.4 SVM with Multiple Classes
We can use the svm() function in a multi-class classification setting, where it will utilize the one-versus-one method.  Let's practice, first by generating a third class of observations in addition to our previous two.
```{r}
set.seed(1)
x <- rbind(x, matrix(rnorm(50*2), ncol = 2))
y <- c(y, rep(0,50))
x[y == 0, 2] <- x[y == 0, 2] + 2
dat <- data.frame(x = x, y = as.factor(y))
par(mfrow = c(1,1))
plot(x, col = (y + 1))
```

The third class of test observations are plotted as black circles. Now, let's fit an SVM to the data:
```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "radial", cost = 10, gamma = 1)
plot(svmfit, dat)
```

Note that we can also use this same e1071 library to perform support vector regression (versus support vector classification) if the response vector passed to svm() is numerical instead of a factor.

### 9.6.5 Application to Gene Expression Data
Here we test our new SVM tools using the Khan data set.  Gene expression measurements were gathered for four types of tumors.  We can observe the dimensions of the data and inspect responses.
```{r}
library(ISLR2)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
```

Let's use a support vector classifier to predict cancer type from the gene expression data.  Since there are much greater features than number of observations, we should use a linear kernel; the flexibility of polynomial or radial kernels is not helpful here.
```{r}
dat <- data.frame(
  x = Khan$xtrain,
  y = as.factor(Khan$ytrain)
)
out <- svm(y ~ ., data = dat, kernel = "linear", cost = 10)
summary(out)
table(out$fitted, dat$y)
```
Wow!  Not a single training error.  Since there are so many variables relative to training observations, it can be easy to find hyperplans which fully separate the classes.  However, as always, we are more concerned about the support vector classifier's performance on test observations.  Let's see how it fares.
```{r}
dat.te <- data.frame(
  x = Khan$xtest,
  y = as.factor(Khan$ytest)
)
pred.te <- predict(out, newdata = dat.te)
table(pred.te, dat.te$y)
```
Our support vector classifier yielded two errors on the test data, both of which it classified a type "2" instead of a type "3".  This yields an accuracy rate of 90%.

### Reflection
Support vector machines (SVMs) are useful classification models because their varying degrees of flexibility through the use of kernels.  The lessons we learned from the regression setting generally apply to the SVM setting.  For example, we should being wary of overfitting, staying conscious of the bias-variance trade-off in choosing a value of the "cost" of margin violations.  The e1071 library provides intuitive tools for leveraging SVMs, including built-in kernels and cross-validation. 

## 4 ISLR Problem 9.6

## 5 Murphy Chapter 18 Summary

## 6 Alzheimer's data Problem

```{r}
# Libraries
library(tidymodels)
library(tidyverse)
library(hash)
library(doParallel)
library(ranger)
library(baguette)
library(xgboost)
library(mltools)
```


```{r}
# Read in data
alz <- read.csv("alzheimer_data.csv")

head(alz)

dim(alz)

#summary(alz)
```

```{r}
# Get Count Of Unique Results (if the count is low its probably a factor) (This is used below in the data cleaning section)

# The reason for this is because there are many variables that are integers that should be factors and I am too lazy to manually convert them to factors so this does it for me based on the threshold of unique values
countResults <- hash()
for (var in colnames(alz)) {
  countResults[[var]] <- nrow(unique(alz[var]))
}

# Note from the results below it seems having unique elements less than 10 is a good enough threshold to determine if a variable is a factor or not
print(countResults)

factoredVars <- character()
for (var in colnames(alz)) {
  if(nrow(unique(alz[var])) <= 10) {
    factoredVars <- append(factoredVars, var)
  }
}

print(factoredVars)
```


```{r}
# Data cleaning (mainly just converting ints to factors)
alz <- alz %>% 
  select(-id) %>% 
  mutate_at(factoredVars, factor)
```


```{r}
# Split into training and testing
set.seed(123)
split <- initial_split(alz)
train_df <- training(split)
test_df <- testing(split)
```

## Random Forest

```{r}
# Random Forest

# The reason for not tuning trees is because it takes too long
spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger")

recipe <- recipe(diagnosis ~ ., data = train_df)

wf <- workflow() %>%
  add_recipe(recipe) %>% 
  add_model(spec)

folds <- vfold_cv(train_df, v = 10)
```

```{r}
registerDoParallel()

tune_res <- tune_grid(
  wf,
  resamples = folds,
  grid = 10
)

tune_res
```

```{r}
best_auc <- select_best(tune_res, "roc_auc")

final_rf <- finalize_model(
  spec,
  best_auc
)

final_rf
```

```{r}
set.seed(123)
final_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_rf)

final_res <- final_wf %>%
  last_fit(split)

final_res %>%
  collect_metrics()
```
**The tuned random forest model produces test misclassification rate of 0.1540741**

## Bagging

```{r}
# Bagging
spec_bagging <- bag_tree(
  mode = "classification",
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()) %>% 
  set_engine("rpart")

wf_bagging <- workflow() %>%
  add_recipe(recipe) %>% 
  add_model(spec_bagging)
```

```{r}
registerDoParallel()

tune_res_bagging <- tune_grid(
  wf_bagging,
  resamples = folds,
  grid = 10
)

tune_res_bagging
```


```{r}
best_auc_bagging <- select_best(tune_res_bagging, "roc_auc")

final_bag <- finalize_model(
  spec_bagging,
  best_auc_bagging
)

final_bag
```

```{r}
set.seed(123)
final_wf_bag <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_bag)

final_res_bag <- final_wf_bag %>%
  last_fit(split)

final_res_bag %>%
  collect_metrics()
```

**The tuned bagging model produces test misclassification rate of 0.1688889**


## Boosting
```{r}
# Boosting

# The reason for not tuning trees is because it takes too long
spec_boosting <- boost_tree(
  mode = "classification",
  mtry = tune(),
  trees = 500,
  min_n = tune()) %>% 
  set_engine("xgboost")

# Since boosting (specifically xgboost) cant handle factors, convert the categorical variables into onehot encoded values
onehot_alz <- data.table::data.table(alz) %>% one_hot(cols = factoredVars[2:length(factoredVars)])
  
# Split into training and testing again with the onehot encoded data
set.seed(123)
split <- initial_split(onehot_alz)
train_df <- training(split)
test_df <- testing(split)

folds <- vfold_cv(train_df, v = 10)

recipe_boost <- recipe(diagnosis ~ ., data = train_df) 

wf_boosting <- workflow() %>%
  add_recipe(recipe_boost) %>% 
  add_model(spec_boosting)
```

```{r}
registerDoParallel()

tune_res_boosting <- tune_grid(
  wf_boosting,
  resamples = folds,
  grid = 10
)

tune_res_boosting
```

```{r}
best_auc_boosting <- select_best(tune_res_boosting, "roc_auc")

final_boost <- finalize_model(
  spec_boosting,
  best_auc_boosting
)

final_boost
```

```{r}
set.seed(123)
final_wf_boost <- workflow() %>%
  add_recipe(recipe_boost) %>%
  add_model(final_boost)

final_res_boost <- final_wf_boost %>%
  last_fit(split)

final_res_boost %>%
  collect_metrics()
```

**The tuned boosting model produces test misclassification rate of 0.157037**


**Comparing the three models: Random forest, bagging and boosting had misclassification rate of 0.1540741, 0.1688889, and 0.157037 respectively. Therefore, the Random Forest was the best model when comparing misclassification rate.**




