---
title: "Midterm Project"
author: "Brandon Amaral, Monte Davityan, Nicholas Lombardo, Hongkai Lu"
date: "10/18/2022"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
options(knitr.kable.NA = '')
```

## 1 ISLR Chapter 9 Summary

### 9.1 Maximal Margin Classifier 

This section is focus on definition and introduction the concept of an optimal separating hyperplane.

#### 9.1.1 What Is a Hyperplane?
\
A line is a hyperplane in a two dimensions space; a plane is a hyperplane in a three dimensions space; a flat affine subspace of p-1 dimension subspace is a hyperplane in a p-dimensional space. 
The mathematical definition of a hyperplane is defined by the equation as follow:
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}=0
$$
We have below sense for a p-dimensional hyperplane.

1: A point X lies on the hyperplane if $ X = (X_{1},X_{2},\dots,X_{p})^T$ in p-dimensional space sattisfies.

2: A point X lies to one side of the hyperplane if 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}>0
$$

3: A point X lies to other side of the hyperplane if 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}<0
$$
Thus, we can think of the hyperplane as dividing p-dimensional space into two halves.

#### 9.1.2 Classification Using a Separating Hyperplane
\
First, suppose that we have a n X p data matrix $\mathbf X$ that consists of n training observations in a p-dimensinal space. The training observations fall into two groups, where $ y_{1}, \dot, y_{n} \in {-1,1} $ and âˆ’1 represents one class and 1 the other class. 
Second, try to constuct a separating hyperplane which is a hyperplane that separates the training observations perfectly according to their class labels. That separating hyperplane has below property:

1: 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}>0 \quad if \quad y_{i}=1,
$$
2: 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}<0  \quad if \quad y_{i}=-1,
$$
Next, we use the constructed separating hyperplane to construct a very natural classifier: a test observation is assigned a class depending on which side of the hyperplane it is located. The classifier is based on a separating hyperplane leads to a linear decision boundary.


#### 9.1.3 The Maximal Margin Classifier 
#### 9.1.4 Construction of the Maximal Margin Classifier 
#### 9.1.5 The Non-separable Case 

### 9.2 Support Vector Classifiers
#### 9.2.1 Overview of the Support Vector Classifier 
#### 9.2.2 Details of the Support Vector Classifier 

### 9.3 Support Vector Machines 
#### 9.3.1 Classification with Non-Linear Decision Boundaries 
#### 9.3.2 The Support Vector Machine 
#### 9.3.3 An Application to the Heart Disease Data 

### 9.4 SVMs with More than Two Classes 
#### 9.4.1 One-Versus-One Classification Dai
#### 9.4.2 One-Versus-All Classification

### 9.5 Relationship to Logistic Regression 

## 2 Murphy Section 17.3 Summary

### 17.3.1 Large margin classifiers
### 17.3.2 The dual problem 
### 17.3.3 Soft margin classifiers 
### 17.3.4 The kernel trick 586
### 17.3.5 Converting SVM outputs into probabilities 
### 17.3.6 Connection with logistic regression 
### 17.3.7 Multi-class classification with SVMs 
### 17.3.8 How to choose the regularizer C 
### 17.3.9 Kernel ridge regression 
### 17.3.10 SVMs for regression 
 
## 3 ISLR Section 9.6 Lab Repo

### 9.6.1 Support Vector Classifier
```{r}
set.seed (1)
x=matrix (rnorm (20*2) , ncol =2)
y=c(rep (-1,10) , rep (1 ,10) )
x[y==1 ,]= x[y==1,] + 1
plot(x, col =(3-y))
```

```{r}
dat=data.frame(x=x, y=as.factor (y))

library (e1071)
svmfit <- svm (y ~ ., data = dat , kernel = "linear", cost = 10 , scale = FALSE )
plot(svmfit , dat)
svmfit$index
```

```{r}
summary (svmfit )
svmfit =svm(y ~ ., data=dat , kernel ="linear", cost =0.1, scale =FALSE )
plot(svmfit , dat)
svmfit$index
```
```{r}
set.seed(1)
tune.out=tune(svm ,y ~ .,data=dat ,kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
```

```{r}
xtest=matrix (rnorm (20*2) , ncol =2)
ytest=sample (c(-1,1) , 20, rep=TRUE)
xtest[ytest ==1 ,]= xtest[ytest ==1,] + 1
testdat =data.frame (x=xtest , y=as.factor (ytest))
```

```{r}
ypred=predict (bestmod ,testdat )
table(predict =ypred , truth= testdat$y )
svmfit =svm(y ~ ., data=dat , kernel ="linear", cost =.01, scale =FALSE )
ypred=predict (svmfit ,testdat )
table(predict =ypred , truth= testdat$y )
```

```{r}
x[y==1 ,]= x[y==1 ,]+0.5
plot(x, col =(y+5) /2, pch =19)
dat=data.frame(x=x,y=as.factor (y))
svmfit =svm(y ~ ., data=dat , kernel ="linear", cost =1e5)
summary (svmfit )
plot(svmfit , dat)
```

```{r}
svmfit =svm(y ~ ., data=dat , kernel ="linear", cost =1)
summary (svmfit )
plot(svmfit ,dat )
```

### 9.6.2 Support Vector Machine
```{r}
set.seed (1)
x=matrix (rnorm (200*2) , ncol =2)
x[1:100 ,]=x[1:100 ,]+2
x[101:150 ,]= x[101:150 ,] -2
y=c(rep (1 ,150) ,rep (2 ,50) )
dat=data.frame(x=x,y=as.factor (y))
plot(x, col=y)
```

```{r}
train=sample (200 ,100)
svmfit =svm(y~., data=dat[train ,], kernel ="radial", gamma =1, cost =1)
plot(svmfit , dat[train ,])
summary(svmfit )

```

```{r}
svmfit =svm(y~., data=dat [train ,], kernel ="radial",gamma =1, cost=1e5)
plot(svmfit ,dat [train ,])
```

```{r}
set.seed (1)
tune.out=tune(svm , y~., data=dat[train ,], kernel ="radial",
ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000), gamma=c(0.5,1,2,3,4) ))
summary (tune.out)
```

```{r}
table(true=dat[-train ,"y"], pred=predict (tune.out$best.model , newdata =dat[-train ,]))
```


### 9.6.3 ROC Curves
```{r}
library (ROCR)
rocplot =function (pred , truth , ...){
 predob = prediction (pred , truth )
 perf = performance (predob , "tpr", "fpr")
 plot(perf ,...)}
```

```{r}
svmfit.opt=svm(y~., data=dat[train ,], kernel ="radial", gamma =2, cost=1, decision.values =TRUE)
fitted =attributes(predict(svmfit.opt ,dat[train ,], decision.values =TRUE))$decision.values
par(mfrow =c(1,2))
rocplot (fitted ,dat [train ,"y"], main="Training Data")
```

```{r}
svmfit.flex=svm (y~., data=dat[train,],kernel ="radial",gamma =50, cost=1, decision.values =TRUE)
fitted <- attributes(predict(svmfit.flex , dat[ train , ], decision.values = T))$decision.values
rocplot(-fitted, dat[train, "y"], add = T, col = "red")
```

```{r}
fitted =attributes (predict (svmfit.opt ,dat[-train ,], decision.values =T))$decision.values
rocplot (fitted ,dat[-train ,"y"], main ="Test Data")
fitted =attributes (predict (svmfit.flex ,dat[-train ,], decision.values =T))$decision.values
rocplot (fitted ,dat[-train ,"y"], add=T,col ="red")
```

### 9.6.4 SVM with Multiple Classes
```{r}
set.seed (1)
x=rbind(x, matrix (rnorm (50*2) , ncol =2))
y=c(y, rep (0 ,50) )
x[y==0 ,2]= x[y==0 ,2]+2
dat=data.frame(x=x, y=as.factor (y))
par(mfrow =c(1,1))
plot(x,col =(y+1))
```

```{r}
svmfit =svm(y~., data=dat , kernel ="radial", cost =10, gamma =1)
plot(svmfit , dat)
```

### 9.6.5 Application to Gene Expression Data
```{r}
library (ISLR)
names(Khan)
dim(Khan$xtrain )
dim(Khan$xtest )
length(Khan$ytrain )
length(Khan$ytest )
```

```{r}
table(Khan$ytrain )
table(Khan$ytest )
```

```{r}
dat=data.frame(x=Khan$xtrain , y=as.factor ( Khan$ytrain ))
out=svm(y~., data=dat , kernel ="linear",cost =10)
summary (out)
table(out$fitted , dat$y)
```

```{r}
dat.te=data.frame(x=Khan$xtest , y=as.factor (Khan$ytest ))
pred.te=predict (out , newdata =dat.te)
table(pred.te , dat.te$y)
```

## 4 ISLR Problem 9.6

## 5 Murphy Chapter 18 Summary

## 6 Alzheimer's data Problem







