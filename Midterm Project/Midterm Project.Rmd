---
title: "Midterm Project"
author: "Brandon Amaral, Monte Davityan, Nicholas Lombardo, Hongkai Lu"
date: "10/18/2022"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
options(knitr.kable.NA = '')
```

## 1 ISLR Chapter 9 Summary

### 9.1 Maximal Margin Classifier 

This section is focus on definition and introduction the concept of an optimal separating hyperplane.

#### 9.1.1 What Is a Hyperplane?
\
A line is a hyperplane in a two dimensions space; a plane is a hyperplane in a three dimensions space; a flat affine subspace of p-1 dimension subspace is a hyperplane in a p-dimensional space. 
The mathematical definition of a hyperplane is defined by the equation as follow:
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}=0
$$
We have below sense for a p-dimensional hyperplane.

1: A point X lies on the hyperplane if $ X = (X_{1},X_{2},\dots,X_{p})^T$ in p-dimensional space sattisfies.

2: A point X lies to one side of the hyperplane if 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}>0
$$

3: A point X lies to other side of the hyperplane if 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}<0
$$
Thus, we can think of the hyperplane as dividing p-dimensional space into two halves.

#### 9.1.2 Classification Using a Separating Hyperplane
\
First, suppose that we have a n X p data matrix $\mathbf X$ that consists of n training observations in a p-dimensinal space. The training observations fall into two groups, where $ y_{1}, \dots, y_{n} \in {-1,1} $ and −1 represents one class and 1 the other class. 
Second, try to constuct a separating hyperplane which is a hyperplane that separates the training observations perfectly according to their class labels. That separating hyperplane has below property:

1: 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}>0 \quad if \quad y_{i}=1,
$$
2: 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}<0  \quad if \quad y_{i}=-1,
$$
Next, we use the constructed separating hyperplane to construct a very natural classifier: a test observation is assigned a class depending on which side of the hyperplane it is located. The classifier is based on a separating hyperplane leads to a linear decision boundary.


#### 9.1.3 The Maximal Margin Classifier 
\
The goal is to construct a classifier based upon a separating hyperplane. Instinctively, the maximal margin hyperplane is the separating hyperplane that optimal separating hyperplane is farthest from the training observations. Then, we can classify a test observation based on which side of the maximal margin hyperplane it lies, which is the maximal margin classifier.

#### 9.1.4 Construction of the Maximal Margin Classifier 
\
Suppose we have a set of n training observations $ x_{1}, \dots, x_{n} \in \mathbb{R}^p $ and associated class labels $ y_{1}, \dots, y_{n} \in {-1,1} $ , then the maximal margin hyperplane is to solve the optimization problem as follows:
$$
  (9.9): \quad \underset {\beta_{0},\beta_{1},\dots,\beta_{p},M} {maximize} M
$$
$$
  (9.10): \quad subject \quad to \quad \sum_{j=1}^{p}\beta_{j}^2 =1
$$
$$
  (9.11): \quad y_{i}(\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\dots+\beta_{p}x_{ip}) \geq M \quad \forall i=1,\dots,n)
$$
\
First, use the inequation (9.11) to constrain each observation liles on the correct side of the hyperplane. Second, both (9.10) and (9.11) ensure that each observation is on the correct side of the hyperplane and at least a distance M from the hyperplane. Last, M represents the margin of our hyperplane, and the
optimization problem chooses $\beta_{0},\beta_{1},\dots,\beta_{p}$ to maximize M.


#### 9.1.5 The Non-separable Case 
\
If a separating hyperplane exists, we can use the maximal margin classifier to perform classification. But in many cases, there is no separating hyperplane. The next section is focus on how to generalize the maximal margin classifier to the non-separable case, which is known as the support vector classifier.

#### 9.2.1 Overview of the Support Vector Classifier
\
Suppose we have two classes observations. Instead of perfectly separating the two classes, the support vector classifier is more interesting on the greater robustness to individual observations and better classification of most of the training observations. In other words, the support vector classifier allows some observations to be lie on the incorrect side of the margin or even the incorrect side of the hyperplane. 

#### 9.2.2 Details of the Support Vector Classifier 
\
Similar to "Construction of the Maximal Margin Classifier", then the Support Vector Classifier is to solve the optimization problem as follows:
$$
  (9.12): \quad \underset {\beta_{0},\beta_{1},\dots,\beta_{p},\epsilon_{1},\dots,\epsilon_{n},M} {maximize} M
$$
$$
  (9.13): \quad subject \; to \; \sum_{j=1}^{p}\beta_{j}^2 =1
$$
$$
  (9.14): \quad y_{i}(\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\dots+\beta_{p}x_{ip}) \geq M(1-\epsilon_{i}) \quad \forall i=1,\dots,n)
$$
$$
  (9.15): \epsilon_{i} \geq 0, \quad \sum_{i=1}^{n}\epsilon_{i}\leq C, where \; C \; is \; a \; nonnegative \; tuning \; parameter
$$
\
First, we use the slack variable $ \epsilon_{i} $ to locate the ith observation relative to the hyperplane and relative to the margin as follow:
\
(1): If $\epsilon_{i} = 0$ then the ith observation is on the correct side of the margin
\
(2): If $\epsilon_{i} \geq 0$ then the ith observation is on the wrong side of the margin
\
(3): If $\epsilon_{i} \geq 1$ then it is on the wrong side of the hyperplane.
\
\
Then, we use the C in (9.15) to be the controller as bias-variance trade-oﬀ of the statistical learning technique. According to the text, we know "When C is small, we seek narrow margins that are rarely violated; this amounts to a classifier that is highly fit to the data, which may have low bias but high variance. On the other hand, when C is larger, the margin is wider and we allow more violations to it; this amounts to fitting the data less hard and obtaining a classifier that is potentially more biased but may have lower variance."
\
\
At last, the observations that lie directly on the margin or on the wrong side of the margin for their class are support vectors, which affect and result the support vector classifier. 

### 9.3 Support Vector Machines 
#### 9.3.1 Classification with Non-Linear Decision Boundaries 

In the two-class setting, the support vector classifier is a natural approach  for classification if the boundary between the two classes is liner. For non-linear class boundaries, we could enlarge the feature space using quadratic, cubic, and even higher-order polynomial functions of the predictors to fit a support vector classifier. 
\
\
Suppose we have a set of n training observations $ x_{1}, \dots, x_{n} \in \mathbb{R}^p $ and associated class labels $ y_{1}, \dots, y_{n} \in {-1,1} $, we use 2p features to fit (9.12) - (9.15)
$$
  X_{1},X_{1}^2,X_{2},X_{2}^2,\dots,X_{p},X_{p}^2.
$$
Then, we have
$$
  (9.16): \quad \underset {\beta_{0},\beta_{11},\beta_{12},\dots,\beta_{p1},\beta_{p2},\epsilon_{1},\dots,\epsilon_{n},M} {maximize} M
$$
$$
  subject \; to \; y_{i}(\beta_{0}+\sum_{j=1}^{p}\beta_{j1}x_{ij}^2+\sum_{j=1}^{p}\beta_{j2}x_{ij}^2) \geq M(1-\epsilon_{i})
$$
$$
  \sum_{i=1}^{n}\epsilon_{i}\leq C, \quad \epsilon_{i} \geq 0, \quad \sum_{j=1}^{p}\sum_{k=1}^{2}\beta_{jk}^2=1
$$

#### 9.3.2 The Support Vector Machine 

By using kernels, the extension of the support vector classifier that results from enlarging the feature space in a specific way is known as The support vector machine (SVM). Suppose we have a support vector classifier can be represented as
$$
  f(x) = \beta_{0}+\sum_{i \in S}\alpha_{i} K\langle x, x_{i}\rangle
$$
(1): If we take a linear kernel $$ K\langle x, x_{i}\rangle = \sum_{j=1}^{p}x_{ij}x_{i'j}  $$  , it is a linear support vector classifier.
\
(2): If we take a polynomial kernel of degree d  $$  K\langle x, x_{i}\rangle = (1 + \sum_{j=1}^{p}x_{ij}x_{i'j})^d  $$  , it is a non-linear support vector classifier.
\
(3): If we take a radial kernel $$ K\langle x, x_{i}\rangle = exp(-\gamma \sum_{j=1}^{p}(x_{ij}-x_{i'j})^2)  $$  , it is a non-linear support vector classifier.

#### 9.3.3 An Application to the Heart Disease Data 

In this section, we fit LDA and SVM to the heart disease training data. By using ROC curves, we can see the SVM using a polynomial kernel of degree d = 1 is slightly superior than LDA. Also, Figure 9.10 displays ROC curves for SVMs using a radial kernel with three values of $ \gamma $, which indicates while a more flexible method will often produce lower training error rates and that does not improve the performance on test data. 

### 9.4 SVMs with More than Two Classes 

The goal is to extend SVMs to the more general case where we have some arbitrary number of classes. The text introduces two methods which are the one-versus-one and one-versus-all approaches. 

#### 9.4.1 One-Versus-One Classification

Suppose there are K > 2 classes, we would like to perform classification using a one-versus-one or all-pairs approach constructs $\binom{k}{2}$ SVMs. 
First, classify a test observation using each of the $\binom{k}{2}$ classifiers. Second, tally the number of times that the test observation is assigned to each
of the K classes. Last, assign the test observation to the class to which it was most frequently assigned in these $\binom{k}{2}$ pairwise classifications to obtain the final classification.

#### 9.4.2 One-Versus-All Classification

In the one-versus-all approach, when we fit K SVMs, we compare one of the K classes to the remaining K-1 classes each time. 

### 9.5 Relationship to Logistic Regression 

First, we have below ridge regression and the lasso function:
$$
  L(X, y, \beta) = \sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}x_{ij}\beta_{j})^2
$$

Then, we introduce the loss function of SVM:
$$
  L(X, y, \beta) = \sum_{i=1}^{n} max[0,1 - y_{i}(\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\dots+\beta_{p}x_{ip})]
$$

The hinge loss function of SVM is closely related to the loss function used in logistic regression. Due to the similarities, the results of logistic regression and the support vector classifier are often similar. SVMs tend to behave better than logistic regression if the classes are well separated; while as, logistic regression is often preferred if there is in more overlapping regimes.

## 2 Murphy Section 17.3 Summary
### 17.3.1 Large margin classifiers
### 17.3.2 The dual problem 
### 17.3.3 Soft margin classifiers 
### 17.3.4 The kernel trick 586
### 17.3.5 Converting SVM outputs into probabilities 
### 17.3.6 Connection with logistic regression 
### 17.3.7 Multi-class classification with SVMs 
### 17.3.8 How to choose the regularizer C 
### 17.3.9 Kernel ridge regression 
### 17.3.10 SVMs for regression  
 
## 3 ISLR Section 9.6 Lab Report
In this lab, we demonstrate the support vector classifier and the support vector machine (SVM).

### 9.6.1 Support Vector Classifier
To start, we generate observations belonging to classes and visually inspect whether they are linearly separable.
```{r}
set.seed(1)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1,10), rep(1,10))
x[y == 1, ] <- x[y == 1, ] +1
plot(x, col = (3 - y))
```

We see they are not linearly separable, so a maximal margin classifier is not usable.  Let's set up a support vector classifier.  To do so, we need to encode the response as a factor variable.  We'll create a data fram with the response coded as a factor.  Then we use the svmfit() function from the e1071 library.
```{r}
dat <- data.frame(x = x, y = as.factor(y))
library(e1071)
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
```

Although we chose not to scale the data here, we may choose to do so in some contexts.  Let's plot the support vector classifier.
```{r}
plot(svmfit, dat)
```

We see that the support vectors are plotted as crosses while the other observations are plotted as circles.  We can determine the identities of the support vectors using:
```{r}
svmfit$index
```

If we want other information about our support vector classifier fit, the summary() command helps us:
```{r}
summary(svmfit)
```

Let's see how our fit is affected by using a smaller value of "cost", which reduces the penalty of margin violations and widens the margins.
```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 0.1, scale = FALSE)
plot(svmfit, dat)
svmfit$index
```

Now we have a greater number of support vectors (which outnumber the other observations!) and we see the margins are noticeably wider.  The svm() function does not explicitly output the coefficients of the linear decision boundary or width of the margin, so we can't verify these quickly.  We can use the tune() function of the e1071 library to perform cross-validation.  Let's do this with a range of values for the "cost" parameter.
```{r}
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat, kernel = "linear",
                 ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
```

The lowest cross-validation error rate was obtained when cost = 0.1.  Conveniently, the best model is stored in the tune() function, accessed here:
```{r}
bestmod <- tune.out$best.model
summary(bestmod)
```

Let's use the predict() function to predict class labels for a set of test observations.  We'll starting by generating a test data set again.
```{r}
xtest <- matrix(rnorm(20 * 2), ncol = 2)
ytest <- sample(c(-1,1), 20, rep = TRUE)
xtest[ytest == 1, ] <- xtest[ytest == 1, ] + 1
testdat <- data.frame(x = xtest, y = as.factor(ytest))
```

Now we will use the best model from our cross-validation to made predictions on the test observations.
```{r}
ypred <- predict(bestmod, testdat)
table(predict = ypred, truth = testdat$y)
```

Using that previously optimized value of cost (0.1), 17 of the test observations were correctly classified.  Let's try using a cost of 0.01 instead.
```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 0.01, scale = FALSE)
ypred <- predict(svmfit, testdat)
table(predict = ypred, truth = testdat$y)
```

Now there are 3 additional misclassified observations.  This is in line with our cross-validation which found a higher error rate with the cost value of 0.01 versus 0.1.

So far, we have only considered data sets where the two classes were not linearly separable.  Now let's explore a case where the classes are separable by a hyperplane.  We generate such a data set:
```{r}
x[y == 1, ] <- x[y == 1, ] + 0.5
plot(x, col = (y+5)/2, pch = 19)
```
We see that the observations are linearly separable, but just barely!  In order to produce strictly separated classes (as in a maximal margin classifier), we can set the "cost" value very high.

```{r}
dat <- data.frame(x = x, y = as.factor(y))
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 1e5)
summary(svmfit)
plot(svmfit, dat)
```

The support vector classifier made no errors on the training data, and the minimum number of support vectors (3) was used.  Since we see there are non-support vectors (graphed as circles) near the decision boundary, we know the margin was quite narrow.  Because of this, we can expect the model to perform less well on test data.  Let's make one more model, this time with a smaller value than cost.

```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "linear", cost = 1)
summary(svmfit)
plot(svmfit, dat)
```

Although our model now has a single misclassified training observation, we can expect it to perform better on test data because of the much wider margin and use of additional support vectors.

### 9.6.2 Support Vector Machine (SVM)
To use a non-linear kernal in our SVM, we can set the kernel type to other values, such as "polynomial" or "radial".  These non-linear kernels also require a second argument; we must specify a "degree" for polynomial" and a "gamma" for "radial".

Let's first generate a data with a non-linear class boundary.
```{r}
set.seed(1)
x <- matrix(rnorm(200 * 2), ncol = 2)
x[1:100, ] <- x[1:100, ] + 2
x[101:150, ] <- x[101:150, ] -2
y <- c(rep(1, 150), rep(2, 50))
dat <- data.frame(x = x, y = as.factor(y))
plot(x, col = y)
```

Our data is now randomly split with a clearly non-linear boundary.  It appears that a radial decision bounday may be a good fit since the observations of class 2 are generally flanked by observations of class 1 to the lower left and upper right.  Let's try a radial kernel with a gamma value of 1.
```{r}
train <- sample(200,100)
svmfit <- svm(y ~ ., data = dat[train, ], kernel = "radial", 
              gamma = 1, cost = 1)
plot(svmfit, dat[train, ])
```

This SVM does indeed have a decision boundary that is clearly not linear; it is shaped like a body of water centered around the "class 2" observations.  We can use the summary() function to get the classification counts, but it will unfortunately not count the number of correctly and incorrectly classified training observations for us.
```{r}
summary(svmfit)
```

Our SVM with a radial kernal used 31 support vectors, and we see roughly a half dozen misclassified training observations.  Again, we can reduce the number of training errors by increasing the value of cost, but we expect an over-fit, irregular decision boundary which would likely perform worse with test data.  Let's see what that would look like.
```{r}
svmfit <- svm(y ~ ., data = dat[train, ], kernel = "radial", 
              gamma = 1, cost = 1e5)
plot(svmfit, dat[train, ])
```

Let's again perform cross-validation using the tune() function to select the best choice training parameters.  This time, we will cross-validate across two parameters (gamma and cost) instead of one.  This will test a greater number of potential models (equal to the product of the number of each parameter)
```{r}
set.seed(1)
tune.out <- tune(svm, y ~ ., data = dat[train, ], kernel = "radial", 
                 ranges = list(
                   cost = c(0.1, 1, 10, 100, 1000),
                   gamma = c(0.5, 1, 2, 3, 4)
                 ))
summary(tune.out)
```

The best combination of parameters are cost = 1 and gamma = 0.5.  Let's view the accuracy of the test set predictions using a table.  We can use "-train" as an index to specify the test data in the dataframe by calling all the observations minus the training data.
```{r}
table(
    true = dat[-train, "y"],
    pred = predict(tune.out$best.model, newdata = dat[-train, ])
)
```

This SVM correctly classified 88 out of 100 test observations, a test accuracy rate of 88%.

### 9.6.3 ROC Curves
We can produce Receiver Operator Characteristic (ROC) curves using the ROCR package.  Here we create a function to plot an ROC curve given numerical scores and class labels for each observation.
```{r}
library(ROCR)
rocplot <- function(pred, truth, ...) {
  predob <- prediction(pred, truth)
  perf <- performance(predob, "tpr", "fpr")
  plot(perf, ...)
}
```

While support vector classifiers (and thus SVMs) output class labels for each observation, we can instead obtain fitted values for each observation.  These are the numerical scores used to obtain the class labels.  The sign of each fitted value typically decides which side of the decision boundary an observation lies; positives lie on one side, and negatives lie on the other.  We can obtain these fitted values directly using the "decision.values = TRUE" parameter when fitting with the svm() function.  Then, to plot the ROC curve where negatives correspond to class 1 and positives to class 2, we use the negative of the fitted values.
```{r}
svmfit.opt <- svm(y ~ ., data = dat[train, ],
                  kernel = "radial", gamma = 2, cost = 1, decision.values = T)
fitted1 <- attributes(predict(svmfit.opt, dat[train, ], decision.values = T)
                     )$decision.values
par(mfrow = c(1,2))
rocplot(-fitted1, dat[train, "y"], main = "Training Data")
```

Since we see the ROC curve is hugging the upper left-hand corner of the plot, the infer the SVM is producing generally accurate predictions.  If we increase the gamma value, our fit will be more flexible and hopefully more accurate.
```{r}
svmfit.flex <- svm(y ~ ., data = dat[train, ], 
                   kernel = "radial", gamma = 50, cost = 1, decision.values = T)
fitted2 <- attributes(
  predict(svmfit.flex, dat[train, ], decision.values = T))$decision.values
rocplot(-fitted1, dat[train, "y"], main = "Training Data")
rocplot(-fitted2, dat[train, "y"], col = "red", add = T)
```

An accuracy rate of 100% on training data usually doesn't bode well for test accuracy.  We have probably over-fit the training data with the greater flexibility of gamma = 50. After all, we are really most interested in the level of prediction accuracy on test data.  Let's now plot ROC curves for the test data.
```{r}
fittedtest1 <- attributes(
  predict(svmfit.opt, dat[-train, ], decision.values = T))$decision.values
rocplot(-fittedtest1, dat[-train, "y"], main = "Test Data")
fittedtest2 <- attributes(
  predict(svmfit.flex, dat[-train, ], decision.values = T))$decision.values
rocplot(-fittedtest2, dat[-train, "y"], col = "red", add = T)
```

We do indeed see that the less flexible model with gamma = 2 does outperform the overly flexible model in test prediction accuracy.

### 9.6.4 SVM with Multiple Classes
We can use the svm() function in a multi-class classification setting, where it will utilize the one-versus-one method.  Let's practice, first by generating a third class of observations in addition to our previous two.
```{r}
set.seed(1)
x <- rbind(x, matrix(rnorm(50*2), ncol = 2))
y <- c(y, rep(0,50))
x[y == 0, 2] <- x[y == 0, 2] + 2
dat <- data.frame(x = x, y = as.factor(y))
par(mfrow = c(1,1))
plot(x, col = (y + 1))
```

The third class of test observations are plotted as black circles. Now, let's fit an SVM to the data:
```{r}
svmfit <- svm(y ~ ., data = dat, kernel = "radial", cost = 10, gamma = 1)
plot(svmfit, dat)
```

Note that we can also use this same e1071 library to perform support vector regression (versus support vector classification) if the response vector passed to svm() is numerical instead of a factor.

### 9.6.5 Application to Gene Expression Data
Here we test our new SVM tools using the Khan data set.  Gene expression measurements were gathered for four types of tumors.  We can observe the dimensions of the data and inspect responses.
```{r}
library(ISLR2)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
table(Khan$ytrain)
table(Khan$ytest)
```

Let's use a support vector classifier to predict cancer type from the gene expression data.  Since there are much greater features than number of observations, we should use a linear kernel; the flexibility of polynomial or radial kernels is not helpful here.
```{r}
dat <- data.frame(
  x = Khan$xtrain,
  y = as.factor(Khan$ytrain)
)
out <- svm(y ~ ., data = dat, kernel = "linear", cost = 10)
summary(out)
table(out$fitted, dat$y)
```
Wow!  Not a single training error.  Since there are so many variables relative to training observations, it can be easy to find hyperplanes which fully separate the classes.  However, as always, we are more concerned about the support vector classifier's performance on test observations.  Let's see how it fares.
```{r}
dat.te <- data.frame(
  x = Khan$xtest,
  y = as.factor(Khan$ytest)
)
pred.te <- predict(out, newdata = dat.te)
table(pred.te, dat.te$y)
```
Our support vector classifier yielded two errors on the test data, both of which it classified a type "2" instead of a type "3".  This yields an accuracy rate of 90%.

### Reflection
Support vector machines (SVMs) are useful classification models because their varying degrees of flexibility through the use of kernels.  The lessons we learned from the regression setting generally apply to the SVM setting.  For example, we should being wary of overfitting, staying conscious of the bias-variance trade-off in choosing a value of the "cost" of margin violations.  The e1071 library provides intuitive tools for leveraging SVMs, including built-in kernels and cross-validation. 

## 4 ISLR Exercise 9.6
In this exercise, we explore the test performance of a support vector classifier using a range of "cost" values.  "Cost" represents the penalty for margin violations on the training set.  We are interested to see if support vector classifiers with a large value of cost (which does not misclassify any training observations) will actually perform worse on test data than a support vector classifier with a small value of cost (which misclassifies a few training observations).

a.) First, we generate a data set that is "just barely linearly separable" with a 2-dimensional hyperplane (a line).  We plot the observations with color coding matching classification for visibility. We also plot a line separating the two classifications as a visual demonstration of separability.
```{r}
#Generate a random data set with normalized x1 and x2 values
s <- 17
set.seed(s)
n <- 60
xtrain <- matrix(data = rnorm(n * 2), ncol = 2)

#Assign observations to one of two classifications
ytrain <- c(rep(-1, n/2), rep(1, n/2))

#Displace the observations of one class relative to the other
x1shift = 2.5
x2shift = 2.5
xtrain[1:n/2, 1] <- xtrain[1:n/2, 1] + x1shift
xtrain[1:n/2, 2] <- xtrain[1:n/2, 2] + x2shift

#Plot the observations colored by classification.
plot(xtrain, col = ytrain + 10)

#Plot a boundary line showing the observations are barely linearly separable.
abline(a=5.45, b=-2.4)
```

b.) Now we perform cross-validation on support vector classifiers with a range of cost values.  We will calculate error rates, count the number of misclassified training observations, and determine how the two are related.
```{r}
library(e1071)

#Create data frame of features and responses
traindata <- data.frame(x = xtrain, y = as.factor(ytrain))

#Perform cross-validation on a range of cost values
set.seed(s)
crossvalidation <- tune(svm, y ~ ., data = traindata, kernel = "linear",
                        ranges = list(cost = c(1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 
                                               1e3)))

#Inspect the cross-validation error rate for each value of cost
#summary(crossvalidation)

#Generate individual models for each cost value used in the cross-validation
svm.cost0.001 <- svm(y ~ ., data = traindata, kernel = "linear", cost = 0.001)
svm.cost0.01 <- svm(y ~ ., data = traindata, kernel = "linear", cost = 0.01)
svm.cost0.1 <- svm(y ~ ., data = traindata, kernel = "linear", cost = 0.1)
svm.cost1 <- svm(y ~ ., data = traindata, kernel = "linear", cost = 1)
svm.cost10 <- svm(y ~ ., data = traindata, kernel = "linear", cost = 10)
svm.cost100 <- svm(y ~ ., data = traindata, kernel = "linear", cost = 100)
svm.cost1000 <- svm(y ~ ., data = traindata, kernel = "linear", cost = 1000)

#Generate tables of predicted classifications versus actual classifications
Table0.001 = table(Predictions_Cost_0.001 = predict(svm.cost0.001, traindata),
      Actual = traindata$y)
Table0.01 = table(Predictions_Cost_0.01 = predict(svm.cost0.01, traindata),
      Actual = traindata$y)
Table0.1 = table(Predictions_Cost_0.1 = predict(svm.cost0.1, traindata),
      Actual = traindata$y)
Table1 = table(Predictions_Cost_1 = predict(svm.cost1, traindata),
      Actual = traindata$y)
Table10 = table(Predictions_Cost_10 = predict(svm.cost10, traindata),
      Actual = traindata$y)
Table100 = table(Predictions_Cost_100 = predict(svm.cost100, traindata),
      Actual = traindata$y)
Table1000 = table(Predictions_Cost_1000 = predict(svm.cost1000, traindata),
      Actual = traindata$y)

#Count the number of training error misclassifications for each cost
ErrorCount0.001 = Table0.001[2] + Table0.001[3]
ErrorCount0.01 = Table0.01[2] + Table0.01[3]
ErrorCount0.1 = Table0.1[2] + Table0.1[3]
ErrorCount1 = Table1[2] + Table1[3]
ErrorCount10 = Table10[2] + Table10[3]
ErrorCount100 = Table100[2] + Table100[3]
ErrorCount1000 = Table1000[2] + Table1000[3]

#Calculate training misclassification error rates for each cost
ErrorRate0.001 <- ErrorCount0.001 / n
ErrorRate0.01 <- ErrorCount0.01 / n
ErrorRate0.1 <- ErrorCount0.1 / n
ErrorRate1 <- ErrorCount1 / n
ErrorRate10 <- ErrorCount10 / n
ErrorRate100 <- ErrorCount100 / n
ErrorRate1000 <- ErrorCount1000 / n

#Compare Training Error Count, Training Error Rate,
#and Cross Validation error rate
TrainError = c(Cost_0.001 = ErrorCount0.001,
                          Cost_0.01 = ErrorCount0.01, Cost_0.1 = ErrorCount0.1,
                          Cost_1 = ErrorCount1, Cost_10 = ErrorCount10,
                          Cost_100 = ErrorCount100, Cost_1000 = ErrorCount1000)
TrainErrorRate = c(ErrorRate0.001, ErrorRate0.01, ErrorRate0.1,
                         ErrorRate1, ErrorRate10, ErrorRate100, ErrorRate1000)
CrossValidation = crossvalidation$performance$error

TrainingTable <- cbind(TrainError, TrainErrorRate, CrossValidation)
TrainingTable
```

With the above table, we can compare the training misclassification errors with the cross-validation error rate across a range of cost values.  When the cost value is small, at 0.001, we have 5 training misclassifications, an error rate of 8%.  When cost is high, at 100 or 1000, the error rate falls to zero.  The cross-validation results, however, are not monotonic decreasing.  Generally, as training error misclassification rate falls, the cross-validation error rate first steeply falls and then modestly rises.  The cost value with the minimum cross-validation error is 0.1.

c.) Now that we have examined the performance of our SVMs on training data, let's proceed to work with test data.  First, we will generate a test dataset in the same manner as our training dataset.  Then we will compute the test errors corresponding to each value of cost we considered previously.
``` {r}
#Generate test data in same fashion as training data
set.seed(s+1)
n <- 60
xtest = matrix(rnorm(2*n), ncol = 2)
ytest = c(rep(-1, n/2), rep(1, n/2))
xtest[1:n/2, 1] <- xtest[1:n/2, 1] + x1shift
xtest[1:n/2, 2] <- xtest[1:n/2, 2] + x2shift
#plot(xtest, col = ytest + 11)
testdata <- data.frame(x = xtest, y = as.factor(ytest))

#Compute test errors corresponding to each value of cost considered
TestTable0.001 = table(Predictions_Cost_0.001 = predict(svm.cost0.001,testdata),
      Actual = testdata$y)
TestTable0.01 = table(Predictions_Cost_0.01 = predict(svm.cost0.01, testdata),
      Actual = testdata$y)
TestTable0.1 = table(Predictions_Cost_0.1 = predict(svm.cost0.1, testdata),
      Actual = testdata$y)
TestTable1 = table(Predictions_Cost_1 = predict(svm.cost1, testdata),
      Actual = testdata$y)
TestTable10 = table(Predictions_Cost_10 = predict(svm.cost10, testdata),
      Actual = testdata$y)
TestTable100 = table(Predictions_Cost_100 = predict(svm.cost100, testdata),
      Actual = testdata$y)
TestTable1000 = table(Predictions_Cost_1000 = predict(svm.cost1000, testdata),
      Actual = testdata$y)

#Count the number of test error misclassifications for each cost
TestErrorCount0.001 = TestTable0.001[2] + TestTable0.001[3]
TestErrorCount0.01 = TestTable0.01[2] + TestTable0.01[3]
TestErrorCount0.1 = TestTable0.1[2] + TestTable0.1[3]
TestErrorCount1 = TestTable1[2] + TestTable1[3]
TestErrorCount10 = TestTable10[2] + TestTable10[3]
TestErrorCount100 = TestTable100[2] + TestTable100[3]
TestErrorCount1000 = TestTable1000[2] + TestTable1000[3]

#Calculate test misclassification error rates for each cost
TestErrorRate0.001 <- TestErrorCount0.001 / n
TestErrorRate0.01 <- TestErrorCount0.01 / n
TestErrorRate0.1 <- TestErrorCount0.1 / n
TestErrorRate1 <- TestErrorCount1 / n
TestErrorRate10 <- TestErrorCount10 / n
TestErrorRate100 <- TestErrorCount100 / n
TestErrorRate1000 <- TestErrorCount1000 / n

#Compare test error counts/rate again training error counts/rate
#and cross-validation error rate
TestTable <- cbind(TestError = c(TestErrorCount0.001,
                                      TestErrorCount0.01,
                                      TestErrorCount0.1, TestErrorCount1,
                                      TestErrorCount10, TestErrorCount100,
                                      TestErrorCount1000),
                   TestErrorRate = c(TestErrorRate0.001, TestErrorRate0.01,
                                     TestErrorRate0.1, TestErrorRate1,
                                     TestErrorRate10, TestErrorRate100,
                                     TestErrorRate1000))
CombinedTable <- cbind(TestTable, TrainingTable)
CombinedTable
```
The SVM with a cost value of 0.001 produced the fewest test classification errors at 3%.  This model was the most flexible of the seven we trained previously.  On the other hand, the SVMs with cost values of 100 and 1000 produced the most classification errors on our test data, a 13% test error rate.  Notably, these results are in contrast to the training error performance where the relationship between cost and classification error rate was reversed: higher costs had yielded lower training error rates.

We plot the SVM with cost = 0.001 here.  Notice that every observation is a support vector.
```{r}
plot(svm.cost0.001, testdata)
```

The cross-validation error rate was exceptionally high at 60% for a cost value of 0.001, yet this cost value produced the lowest test error rate.  Our cross-validation suggested a cost-value of 0.1 was optimal, yet this value produced a modestly high test error rate.

d.)  The dissonance between classification error rates on the training data and test data highlights the dangers of strictly fitting data which is just barely linearly separable.  When a separating hyperplane depends on just a few observations, the support vector classifier is subject to high variance and overfitting.  Both the test error rate and cross-validation rates for high cost values support this.  

Interestingly, the test error rate and cross-validation error rate do not support the same cost values, though they both provide evidence that a high cost model is not best.  Cross-validation supports a moderate cost of 0.1 while the test error rate supports a very low cost of 0.001.  As mentioned before, the cross-validation had a very high error rate of over 50% for the 0.001 cost model.  What could be happening here?

We believe part of the discrepancy is due to the relatively small sample sizes (n=60) used in the training and test data.  Creating randomly generated data sets that were "just barely linearly separable" with large sample sizes tended to require the center of each classification of observations to be significantly displaced, by 4 or more standard deviations.  This yielded SVMs whose accuracy were less prone to changes in the hyperplane slope, yielding more homogeneous results for varying degrees of cost.  We can now leverage the law of large numbers by creating a very large test data set to get a better sense of the average power at each cost value.  Let us try a test sample size of n=10000.

``` {r, echo=FALSE}
#Generate test data in same fashion as training data
set.seed(s+1)
n <- 10000
xtest = matrix(rnorm(2*n), ncol = 2)
ytest = c(rep(-1, n/2), rep(1, n/2))
xtest[1:n/2, 1] <- xtest[1:n/2, 1] + x1shift
xtest[1:n/2, 2] <- xtest[1:n/2, 2] + x2shift
#plot(xtest, col = ytest + 11)
testdata <- data.frame(x = xtest, y = as.factor(ytest))

#Compute test errors corresponding to each value of cost considered
TestTable0.001 = table(Predictions_Cost_0.001 = predict(svm.cost0.001,testdata),
      Actual = testdata$y)
TestTable0.01 = table(Predictions_Cost_0.01 = predict(svm.cost0.01, testdata),
      Actual = testdata$y)
TestTable0.1 = table(Predictions_Cost_0.1 = predict(svm.cost0.1, testdata),
      Actual = testdata$y)
TestTable1 = table(Predictions_Cost_1 = predict(svm.cost1, testdata),
      Actual = testdata$y)
TestTable10 = table(Predictions_Cost_10 = predict(svm.cost10, testdata),
      Actual = testdata$y)
TestTable100 = table(Predictions_Cost_100 = predict(svm.cost100, testdata),
      Actual = testdata$y)
TestTable1000 = table(Predictions_Cost_1000 = predict(svm.cost1000, testdata),
      Actual = testdata$y)

#Count the number of test error misclassifications for each cost
TestErrorCount0.001 = TestTable0.001[2] + TestTable0.001[3]
TestErrorCount0.01 = TestTable0.01[2] + TestTable0.01[3]
TestErrorCount0.1 = TestTable0.1[2] + TestTable0.1[3]
TestErrorCount1 = TestTable1[2] + TestTable1[3]
TestErrorCount10 = TestTable10[2] + TestTable10[3]
TestErrorCount100 = TestTable100[2] + TestTable100[3]
TestErrorCount1000 = TestTable1000[2] + TestTable1000[3]

#Calculate test misclassification error rates for each cost
TestErrorRate0.001 <- TestErrorCount0.001 / n
TestErrorRate0.01 <- TestErrorCount0.01 / n
TestErrorRate0.1 <- TestErrorCount0.1 / n
TestErrorRate1 <- TestErrorCount1 / n
TestErrorRate10 <- TestErrorCount10 / n
TestErrorRate100 <- TestErrorCount100 / n
TestErrorRate1000 <- TestErrorCount1000 / n

#Compare test error counts/rate again training error counts/rate
#and cross-validation error rate
TestTable <- cbind(TestError = c(TestErrorCount0.001,
                                      TestErrorCount0.01,
                                      TestErrorCount0.1, TestErrorCount1,
                                      TestErrorCount10, TestErrorCount100,
                                      TestErrorCount1000),
                   TestErrorRate = c(TestErrorRate0.001, TestErrorRate0.01,
                                     TestErrorRate0.1, TestErrorRate1,
                                     TestErrorRate10, TestErrorRate100,
                                     TestErrorRate1000))
CombinedTable <- cbind(TestTable, TrainingTable)
CombinedTable
```

On a much larger test data set, we see that SVMs with cost values of 0.1 and 0.01 perform better the 0.001 cost.  This outcome is closer to what we would have expected based on our cross-validation which favored a 0.1 cost value.  Earlier, the cross-validation results did not correspond as nicely with the test error rate from the smaller test data set.  It seems random chance was playing a significant role in measuring the accuracy of the models on small data sets.  

Cross-validation is great tool for informing our choice of model parameters.  When paired with other decision-making tools, we can generate a model that best meets our needs without falling into common traps such as overfitting training data.  We should be especially be careful in edge cases, such as when data are just barely linearly separable, as we can be misled by performance on these precarious training data.

## 5 Murphy Chapter 18 Summary

## 6 Alzheimer's data Problem

```{r}
# Libraries
library(tidymodels)
library(tidyverse)
library(hash)
library(doParallel)
library(ranger)
library(baguette)
library(xgboost)
library(mltools)
library(ggplot2)
```


```{r}
# Read in data
alz <- read.csv("alzheimer_data.csv")

head(alz)

dim(alz)

#summary(alz)
```

```{r}
# Get Count Of Unique Results (if the count is low its probably a factor) (This is used below in the data cleaning section)

# The reason for this is because there are many variables that are integers that should be factors and I am too lazy to manually convert them to factors so this does it for me based on the threshold of unique values
countResults <- hash()
for (var in colnames(alz)) {
  countResults[[var]] <- nrow(unique(alz[var]))
}

# Note from the results below it seems having unique elements less than 10 is a good enough threshold to determine if a variable is a factor or not
print(countResults)

factoredVars <- character()
for (var in colnames(alz)) {
  if(nrow(unique(alz[var])) <= 10) {
    factoredVars <- append(factoredVars, var)
  }
}

print(factoredVars)
```

```{r}
# Data cleaning (mainly just converting ints to factors)
alz <- alz %>% 
  select(-id) %>% 
  mutate_at(factoredVars, factor)
```



```{r}
### Some basic data exploration on a few predictors

#### Distribution of diagnosis
ggplot(data = alz, mapping = aes(x = diagnosis)) +
  geom_bar()

alz %>% group_by(diagnosis) %>% summarise(prop = n() / nrow(alz))
```


A normal (0) diagnosis is slightly more skewed in terms of frequency of occurrence then the other two diagnosis types. 

```{r}
# Effect of age on diagnosis
ggplot(data = alz, mapping = aes(x = age, y = diagnosis)) +
  geom_boxplot()

alz %>% group_by(diagnosis) %>% summarise(mean_age = mean(age), med_age = median(age), sd_age = sd(age))
```


Seems a diagnosis of normal (0) with regards to age is slightly skewed towards being younger which makes since alzhemiers is a progressive degenerative disease. This is probably a significant predictor.


```{r}
# Effect of mealprep on diagnosis
ggplot(data = alz, mapping = aes(x = diagnosis, color = mealprep, fill = mealprep)) +
  geom_bar()
```


Mealprep of 0 (normal meal prep without assistance) is heavily skewed towards a diagnosis of normal (0) and the frequency of progressively worse meal prep scores increases as the diagnosis worsens implying mealprep may be a significant predictor.


```{r}
# Split into training and testing
set.seed(123)
split <- initial_split(alz)
train_df <- training(split)
test_df <- testing(split)
```

## Random Forest

```{r}
# Random Forest

# The reason for not tuning trees is because it takes too long

# Define the specification for classification for random forest for tidymodels where mtry and min_n will be tuned
spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger")

# Define a recipe for tidymodels
recipe <- recipe(diagnosis ~ ., data = train_df)

# Define the workflow for tidymodels
wf <- workflow() %>%
  add_recipe(recipe) %>% 
  add_model(spec)

# Define the folds for cross validated tuning
folds <- vfold_cv(train_df, v = 10)
```

```{r}
# Register parallel computation
registerDoParallel()

# Tune the model using the folds
tune_res <- tune_grid(
  wf,
  resamples = folds,
  grid = 10
)
```

```{r}
# Select the best model configuration based on roc area under curve
best_auc <- select_best(tune_res, "roc_auc")

# Finalize the best model using the best hyperparameters
final_rf <- finalize_model(
  spec,
  best_auc
)

final_rf
```

```{r}
set.seed(123)
# Finalize the workflow
final_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_rf)

# Finalize the fit
final_res <- final_wf %>%
  last_fit(split)

# Output accuracy and roc_auc
final_res %>%
  collect_metrics()
```
**The tuned random forest model produces test misclassification rate of 0.1540741**

## Bagging

```{r}
# Bagging

# Define the specification for classification for bagging for tidymodels where min_n, tree_depth and cost complexity will be tuned
spec_bagging <- bag_tree(
  mode = "classification",
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()) %>% 
  set_engine("rpart")

# Add the recipe from above and the bagging specification
wf_bagging <- workflow() %>%
  add_recipe(recipe) %>% 
  add_model(spec_bagging)
```

```{r}
# Enable parrallel compute
registerDoParallel()

# Tune the hyperparameters for the bagging model
tune_res_bagging <- tune_grid(
  wf_bagging,
  resamples = folds,
  grid = 10
)
```


```{r}
# Select the best bagging model based on roc area under curve
best_auc_bagging <- select_best(tune_res_bagging, "roc_auc")

# Finalize the model based on the best hyperparameters
final_bag <- finalize_model(
  spec_bagging,
  best_auc_bagging
)

final_bag
```

```{r}
set.seed(123)
# Finalize the workflow
final_wf_bag <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_bag)

# Finalize the fit
final_res_bag <- final_wf_bag %>%
  last_fit(split)

# Output accuracy and roc_auc 
final_res_bag %>%
  collect_metrics()
```

**The tuned bagging model produces test misclassification rate of 0.1688889**


## Boosting
```{r}
# Boosting

# The reason for not tuning trees is because it takes too long

# Define the specification for classification for boosting for tidymodels where mtry and min_n will be tuned
spec_boosting <- boost_tree(
  mode = "classification",
  mtry = tune(),
  trees = 500,
  min_n = tune()) %>% 
  set_engine("xgboost")

# Since boosting (specifically xgboost) cant handle factors, convert the categorical variables into onehot encoded values
onehot_alz <- data.table::data.table(alz) %>% one_hot(cols = factoredVars[2:length(factoredVars)])
  
# Split into training and testing again with the onehot encoded data
set.seed(123)
split <- initial_split(onehot_alz)
train_df <- training(split)
test_df <- testing(split)

# Redefine folds, recipe and workflow
folds <- vfold_cv(train_df, v = 10)

recipe_boost <- recipe(diagnosis ~ ., data = train_df) 

wf_boosting <- workflow() %>%
  add_recipe(recipe_boost) %>% 
  add_model(spec_boosting)
```

```{r}
# Enable parallel compute
registerDoParallel()

# Tune the boosting model
tune_res_boosting <- tune_grid(
  wf_boosting,
  resamples = folds,
  grid = 10
)
```

```{r}
# Select the best boosting hyperparameters based on roc area under curve
best_auc_boosting <- select_best(tune_res_boosting, "roc_auc")

# Finalaize the model
final_boost <- finalize_model(
  spec_boosting,
  best_auc_boosting
)

final_boost
```

```{r}
set.seed(123)
# Finalaize the workflow
final_wf_boost <- workflow() %>%
  add_recipe(recipe_boost) %>%
  add_model(final_boost)

# FInalaize the fit
final_res_boost <- final_wf_boost %>%
  last_fit(split)

# Output accuracy and roc_auc
final_res_boost %>%
  collect_metrics()
```

**The tuned boosting model produces test misclassification rate of 0.157037**


**Comparing the three models: Random forest, bagging and boosting had misclassification rate of 0.1540741, 0.1688889, and 0.157037 respectively. Therefore, the Random Forest was the best model when comparing misclassification rate which means about 15.4% of predictions made were incorrect on the test set.**




