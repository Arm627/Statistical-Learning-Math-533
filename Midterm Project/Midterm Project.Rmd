---
title: "Midterm Project"
author: "Brandon Amaral, Monte Davityan, Nicholas Lombardo, Hongkai Lu"
date: "10/18/2022"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
options(knitr.kable.NA = '')
```

## 1 ISLR Chapter 9 Summary

### 9.1 Maximal Margin Classifier 

This section is focus on definition and introduction the concept of an optimal separating hyperplane.

#### 9.1.1 What Is a Hyperplane?
\
A line is a hyperplane in a two dimensions space; a plane is a hyperplane in a three dimensions space; a flat affine subspace of p-1 dimension subspace is a hyperplane in a p-dimensional space. 
The mathematical definition of a hyperplane is defined by the equation as follow:
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}=0
$$
We have below sense for a p-dimensional hyperplane.

1: A point X lies on the hyperplane if $ X = (X_{1},X_{2},\dots,X_{p})^T$ in p-dimensional space sattisfies.

2: A point X lies to one side of the hyperplane if 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}>0
$$

3: A point X lies to other side of the hyperplane if 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}<0
$$
Thus, we can think of the hyperplane as dividing p-dimensional space into two halves.

#### 9.1.2 Classification Using a Separating Hyperplane
\
First, suppose that we have a n X p data matrix $\mathbf X$ that consists of n training observations in a p-dimensinal space. The training observations fall into two groups, where $ y_{1}, \dot, y_{n} \in {-1,1} $ and âˆ’1 represents one class and 1 the other class. 
Second, try to constuct a separating hyperplane which is a hyperplane that separates the training observations perfectly according to their class labels. That separating hyperplane has below property:

1: 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}>0 \quad if \quad y_{i}=1,
$$
2: 
$$
\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}+\cdots+\beta_{p}X_{p}<0  \quad if \quad y_{i}=-1,
$$
Next, we use the constructed separating hyperplane to construct a very natural classifier: a test observation is assigned a class depending on which side of the hyperplane it is located. The classifier is based on a separating hyperplane leads to a linear decision boundary.


#### 9.1.3 The Maximal Margin Classifier 
#### 9.1.4 Construction of the Maximal Margin Classifier 
#### 9.1.5 The Non-separable Case 

### 9.2 Support Vector Classifiers
#### 9.2.1 Overview of the Support Vector Classifier 
#### 9.2.2 Details of the Support Vector Classifier 

### 9.3 Support Vector Machines 
#### 9.3.1 Classification with Non-Linear Decision Boundaries 
#### 9.3.2 The Support Vector Machine 
#### 9.3.3 An Application to the Heart Disease Data 

### 9.4 SVMs with More than Two Classes 
#### 9.4.1 One-Versus-One Classification Dai
#### 9.4.2 One-Versus-All Classification

### 9.5 Relationship to Logistic Regression 

## 2 Murphy Section 17.3 Summary

### 17.3.1 Large margin classifiers
### 17.3.2 The dual problem 
### 17.3.3 Soft margin classifiers 
### 17.3.4 The kernel trick 586
### 17.3.5 Converting SVM outputs into probabilities 
### 17.3.6 Connection with logistic regression 
### 17.3.7 Multi-class classification with SVMs 
### 17.3.8 How to choose the regularizer C 
### 17.3.9 Kernel ridge regression 
### 17.3.10 SVMs for regression 
 
## 3 ISLR Section 9.6 Lab Repo

### 9.6.1 Support Vector Classifier
```{r}
set.seed (1)
x=matrix (rnorm (20*2) , ncol =2)
y=c(rep (-1,10) , rep (1 ,10) )
x[y==1 ,]= x[y==1,] + 1
plot(x, col =(3-y))
```

```{r}
dat=data.frame(x=x, y=as.factor (y))

library (e1071)
svmfit <- svm (y ~ ., data = dat , kernel = "linear", cost = 10 , scale = FALSE )
plot(svmfit , dat)
svmfit$index
```

```{r}
summary (svmfit )
svmfit =svm(y ~ ., data=dat , kernel ="linear", cost =0.1, scale =FALSE )
plot(svmfit , dat)
svmfit$index
```
```{r}
set.seed(1)
tune.out=tune(svm ,y ~ .,data=dat ,kernel ="linear",
ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100) ))
summary(tune.out)
bestmod = tune.out$best.model
summary(bestmod)
```

```{r}
xtest=matrix (rnorm (20*2) , ncol =2)
ytest=sample (c(-1,1) , 20, rep=TRUE)
xtest[ytest ==1 ,]= xtest[ytest ==1,] + 1
testdat =data.frame (x=xtest , y=as.factor (ytest))
```

```{r}
ypred=predict (bestmod ,testdat )
table(predict =ypred , truth= testdat$y )
svmfit =svm(y ~ ., data=dat , kernel ="linear", cost =.01, scale =FALSE )
ypred=predict (svmfit ,testdat )
table(predict =ypred , truth= testdat$y )
```

```{r}
x[y==1 ,]= x[y==1 ,]+0.5
plot(x, col =(y+5) /2, pch =19)
dat=data.frame(x=x,y=as.factor (y))
svmfit =svm(y ~ ., data=dat , kernel ="linear", cost =1e5)
summary (svmfit )
plot(svmfit , dat)
```

```{r}
svmfit =svm(y ~ ., data=dat , kernel ="linear", cost =1)
summary (svmfit )
plot(svmfit ,dat )
```

### 9.6.2 Support Vector Machine
```{r}
set.seed (1)
x=matrix (rnorm (200*2) , ncol =2)
x[1:100 ,]=x[1:100 ,]+2
x[101:150 ,]= x[101:150 ,] -2
y=c(rep (1 ,150) ,rep (2 ,50) )
dat=data.frame(x=x,y=as.factor (y))
plot(x, col=y)
```

```{r}
train=sample (200 ,100)
svmfit =svm(y~., data=dat[train ,], kernel ="radial", gamma =1, cost =1)
plot(svmfit , dat[train ,])
summary(svmfit )

```

```{r}
svmfit =svm(y~., data=dat [train ,], kernel ="radial",gamma =1, cost=1e5)
plot(svmfit ,dat [train ,])
```

```{r}
set.seed (1)
tune.out=tune(svm , y~., data=dat[train ,], kernel ="radial",
ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000), gamma=c(0.5,1,2,3,4) ))
summary (tune.out)
```

```{r}
table(true=dat[-train ,"y"], pred=predict (tune.out$best.model , newdata =dat[-train ,]))
```


### 9.6.3 ROC Curves
```{r}
library (ROCR)
rocplot =function (pred , truth , ...){
 predob = prediction (pred , truth )
 perf = performance (predob , "tpr", "fpr")
 plot(perf ,...)}
```

```{r}
svmfit.opt=svm(y~., data=dat[train ,], kernel ="radial", gamma =2, cost=1, decision.values =TRUE)
fitted =attributes(predict(svmfit.opt ,dat[train ,], decision.values =TRUE))$decision.values
par(mfrow =c(1,2))
rocplot (fitted ,dat [train ,"y"], main="Training Data")
```

```{r}
svmfit.flex=svm (y~., data=dat[train,],kernel ="radial",gamma =50, cost=1, decision.values =TRUE)
fitted <- attributes(predict(svmfit.flex , dat[ train , ], decision.values = T))$decision.values
rocplot(-fitted, dat[train, "y"], add = T, col = "red")
```

```{r}
fitted =attributes (predict (svmfit.opt ,dat[-train ,], decision.values =T))$decision.values
rocplot (fitted ,dat[-train ,"y"], main ="Test Data")
fitted =attributes (predict (svmfit.flex ,dat[-train ,], decision.values =T))$decision.values
rocplot (fitted ,dat[-train ,"y"], add=T,col ="red")
```

### 9.6.4 SVM with Multiple Classes
```{r}
set.seed (1)
x=rbind(x, matrix (rnorm (50*2) , ncol =2))
y=c(y, rep (0 ,50) )
x[y==0 ,2]= x[y==0 ,2]+2
dat=data.frame(x=x, y=as.factor (y))
par(mfrow =c(1,1))
plot(x,col =(y+1))
```

```{r}
svmfit =svm(y~., data=dat , kernel ="radial", cost =10, gamma =1)
plot(svmfit , dat)
```

### 9.6.5 Application to Gene Expression Data
```{r}
library (ISLR)
names(Khan)
dim(Khan$xtrain )
dim(Khan$xtest )
length(Khan$ytrain )
length(Khan$ytest )
```

```{r}
table(Khan$ytrain )
table(Khan$ytest )
```

```{r}
dat=data.frame(x=Khan$xtrain , y=as.factor ( Khan$ytrain ))
out=svm(y~., data=dat , kernel ="linear",cost =10)
summary (out)
table(out$fitted , dat$y)
```

```{r}
dat.te=data.frame(x=Khan$xtest , y=as.factor (Khan$ytest ))
pred.te=predict (out , newdata =dat.te)
table(pred.te , dat.te$y)
```

## 4 ISLR Problem 9.6

## 5 Murphy Chapter 18 Summary

## 6 Alzheimer's data Problem

```{r}
# Libraries
library(tidymodels)
library(tidyverse)
library(hash)
library(doParallel)
library(ranger)
library(baguette)
library(xgboost)
library(mltools)
```


```{r}
# Read in data
alz <- read.csv("alzheimer_data.csv")

head(alz)

dim(alz)

#summary(alz)
```

```{r}
# Get Count Of Unique Results (if the count is low its probably a factor) (This is used below in the data cleaning section)

# The reason for this is because there are many variables that are integers that should be factors and I am too lazy to manually convert them to factors so this does it for me based on the threshold of unique values
countResults <- hash()
for (var in colnames(alz)) {
  countResults[[var]] <- nrow(unique(alz[var]))
}

# Note from the results below it seems having unique elements less than 10 is a good enough threshold to determine if a variable is a factor or not
print(countResults)

factoredVars <- character()
for (var in colnames(alz)) {
  if(nrow(unique(alz[var])) <= 10) {
    factoredVars <- append(factoredVars, var)
  }
}

print(factoredVars)
```


```{r}
# Data cleaning (mainly just converting ints to factors)
alz <- alz %>% 
  select(-id) %>% 
  mutate_at(factoredVars, factor)
```


```{r}
# Split into training and testing
set.seed(123)
split <- initial_split(alz)
train_df <- training(split)
test_df <- testing(split)
```

## Random Forest

```{r}
# Random Forest

# The reason for not tuning trees is because it takes too long
spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger")

recipe <- recipe(diagnosis ~ ., data = train_df)

wf <- workflow() %>%
  add_recipe(recipe) %>% 
  add_model(spec)

folds <- vfold_cv(train_df, v = 10)
```

```{r}
registerDoParallel()

tune_res <- tune_grid(
  wf,
  resamples = folds,
  grid = 10
)

tune_res
```

```{r}
best_auc <- select_best(tune_res, "roc_auc")

final_rf <- finalize_model(
  spec,
  best_auc
)

final_rf
```

```{r}
set.seed(123)
final_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_rf)

final_res <- final_wf %>%
  last_fit(split)

final_res %>%
  collect_metrics()
```
**The tuned random forest model produces test misclassification rate of 0.1540741**

## Bagging

```{r}
# Bagging
spec_bagging <- bag_tree(
  mode = "classification",
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()) %>% 
  set_engine("rpart")

wf_bagging <- workflow() %>%
  add_recipe(recipe) %>% 
  add_model(spec_bagging)
```

```{r}
registerDoParallel()

tune_res_bagging <- tune_grid(
  wf_bagging,
  resamples = folds,
  grid = 10
)

tune_res_bagging
```


```{r}
best_auc_bagging <- select_best(tune_res_bagging, "roc_auc")

final_bag <- finalize_model(
  spec_bagging,
  best_auc_bagging
)

final_bag
```

```{r}
set.seed(123)
final_wf_bag <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_bag)

final_res_bag <- final_wf_bag %>%
  last_fit(split)

final_res_bag %>%
  collect_metrics()
```

**The tuned bagging model produces test misclassification rate of 0.1688889**


## Boosting
```{r}
# Boosting

# The reason for not tuning trees is because it takes too long
spec_boosting <- boost_tree(
  mode = "classification",
  mtry = tune(),
  trees = 500,
  min_n = tune()) %>% 
  set_engine("xgboost")

# Since boosting (specifically xgboost) cant handle factors, convert the categorical variables into onehot encoded values
onehot_alz <- data.table::data.table(alz) %>% one_hot(cols = factoredVars[2:length(factoredVars)])
  
# Split into training and testing again with the onehot encoded data
set.seed(123)
split <- initial_split(onehot_alz)
train_df <- training(split)
test_df <- testing(split)

folds <- vfold_cv(train_df, v = 10)

recipe_boost <- recipe(diagnosis ~ ., data = train_df) 

wf_boosting <- workflow() %>%
  add_recipe(recipe_boost) %>% 
  add_model(spec_boosting)
```

```{r}
registerDoParallel()

tune_res_boosting <- tune_grid(
  wf_boosting,
  resamples = folds,
  grid = 10
)

tune_res_boosting
```

```{r}
best_auc_boosting <- select_best(tune_res_boosting, "roc_auc")

final_boost <- finalize_model(
  spec_boosting,
  best_auc_boosting
)

final_boost
```

```{r}
set.seed(123)
final_wf_boost <- workflow() %>%
  add_recipe(recipe_boost) %>%
  add_model(final_boost)

final_res_boost <- final_wf_boost %>%
  last_fit(split)

final_res_boost %>%
  collect_metrics()
```

**The tuned boosting model produces test misclassification rate of 0.157037**


**Comparing the three models: Random forest, bagging and boosting had misclassification rate of 0.1540741, 0.1688889, and 0.157037 respectively. Therefore, the Random Forest was the best model when comparing misclassification rate.**




